---
title: "Godaddy Microbusiness Forecasting"
author: "Mohammad Solki"
date: "2023-02-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Set the working directory
setwd("/Users/dreamer/Downloads/Godaddy/godaddy_microbusiness_forecasting")
```

```{r}
# Importing the libraries
library(tidyverse)
# ggplot2, purrr, tibble, dplyr, tidyr, stringr, readr, forcats
library(mice)
library(maps)
#library(ggplot2)
library(gridExtra)
#library(dplyr)
library(caret)
library(gbm)
#library(png)
#library(ggmap)
library(viridis)
library(mapdata)
library(corrplot)


# Code starts here
```

### Exploring the datasets

Explore the datasets to get a better understanding of the data.\
Load the train and test datasets into dataframes.

```{r}
# Load train.csv into a dataframe
train_df <- read.csv("./datasets/train.csv")

# Load test.csv into a dataframe
test_df <- read.csv("./datasets/test.csv")

# Load census_starter.csv into a dataframe
census_df <- read.csv("./datasets/census_starter.csv")

```

Check the dataframes

After reading the CSV files into dataframes, we should check whether the data is loaded correctly or not. We can use the head() function of R to display the first few rows of the dataframes and tail() function to display the last rows. This will display the first and last 10 rows of the train, test and census dataframes. We can also use other pandas functions such as info() and describe() to get more information about the dataframes, such as column names, data types, and summary statistics.

```{r}
# Display the first 10 rows of the dataframes
head(train_df, n = 10)
head(test_df, n = 10)
head(census_df, n = 10)

```

```{r}
# Display the last 10 rows of the dataframes
tail(train_df, n = 10)
tail(test_df, n = 10)
tail(census_df, n = 10)
```

```{r}
# Display information about the train dataframe
str(train_df)
cat(rep("=", 40), "\n") # Print a line of 40 equal signs
summary(train_df)
```

```{r}
# Display information about the test dataframe
str(test_df)
cat(rep("=", 40), "\n") # Print a line of 40 equal signs
summary(test_df)
```

```{r}
# Display information about the census dataframe
str(census_df)
cat(rep("=", 40), "\n") # Print a line of 40 equal signs
summary(census_df)
```

The is.na() function is used to create a logical matrix where TRUE represents a missing value and FALSE represents a non-missing value. The colSums() function is then used to count the number of missing values in each column of the data frame. If the sum of a column is greater than 0, it means that there is at least one missing value in that column.

```{r}
# Check for missing values in the train data frame
colSums(is.na(train_df))
```

```{r fig.width=6}
#{r fig.width=7, fig.align='center', fig.height=4, out.width='100%'}
# Calculate the number and percentage of missing values for each column
missing_data <- census_df %>%
  summarise_all(~ sum(is.na(.))) %>%
  gather(variable, missing_count) %>%
  mutate(missing_percent = missing_count/nrow(census_df)*100)

# Create two plots side by side
plot1 <- ggplot(missing_data, aes(x = missing_count, y = variable)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Number of missing values", y = "") +
  ggtitle("Number of missing values in census_df")

plot2 <- ggplot(missing_data, aes(x = missing_percent, y = variable)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Percentage of missing values", y = "") +
  ggtitle("Percentage of missing values in census_df")

# Arrange the two plots side by side
grid.arrange(plot1, plot2, ncol = 2)
```

```{r}
# Check for missing values in the test data frame
colSums(is.na(test_df))
```

```{r}
# Check for missing values in the census data frame
colSums(is.na(census_df))
```

We use the complete.cases() function to determine which rows have complete data and which rows have missing values. The complete.cases() function returns a logical vector indicating which rows have no missing values. Therefore, to identify the rows with missing values, we use the ! operator to negate the logical vector returned by complete.cases(). Then, we use the is.na() function to identify which columns have missing values for each missing row:

```{r}
# Identify rows with missing values in census_df
missing_rows <- which(!complete.cases(census_df))
```

```{r}
# Identify columns with missing values for each missing row
for (i in missing_rows) {
  cat("Row", i, "has missing values in columns:",
      paste(names(census_df)[is.na(census_df[i,])], collapse = ", "), "\n")
}
print(census_df[missing_rows,])

```

We use the mice package to impute missing values in the census_df dataframe.

m: the number of imputations to generate was set to 5, because, generally, m should be set to at least 5 for good imputation performance. creating too many datasets will increase the computational cost and may not necessarily lead to better results.

maxit: maxit was set to 50 to allow for a larger number of iterations to ensure that the imputation algorithm converges and fills in missing values as accurately as possible.

method: In this case, we are using "pmm" which stands for "Predictive Mean Matching", because it is a flexible and widely used imputation method that works well with continuous variables. The method estimates the missing values by drawing from a set of observed values that have similar characteristics to the missing values.

```{r}
# Impute missing data using mice
imputed_df <- mice(census_df, m = 5, maxit = 50, method = "pmm")
```

```{r}
# Extract imputed data
imputed_data <- complete(imputed_df)

# Check for missing values in imputed data
colSums(is.na(imputed_data))

```

```{r}
print(imputed_data[missing_rows,])
```

```{r}
index <- unique(train_df$first_day_of_month)
print(index)
```

training data is from 08/2019 to 10/2022

```{r}
index <- unique(test_df$first_day_of_month)
print(index)
```

prediction dates are from 11/2022 to 06/2023

## Let's Do EDA

To make analysis easier and be able to group the data by year and month, we'll extract year and month values from the first_day_of_month column in both train and test dataframes using apply() method and lambda function, and then create new columns called year (int), month (int), and year_month (str) in each dataframe to store these values.

```{r}
# Add year, month and year_month columns to train_df
train_df$year <- as.integer(substr(train_df$first_day_of_month, 1, 4))
train_df$month <- as.integer(substr(train_df$first_day_of_month, 6, 7))
train_df$year_month <- substr(train_df$first_day_of_month, 1, 7)

# Add year, month and year_month columns to test_df
test_df$year <- as.integer(substr(test_df$first_day_of_month, 1, 4))
test_df$month <- as.integer(substr(test_df$first_day_of_month, 6, 7))
test_df$year_month <- substr(test_df$first_day_of_month, 1, 7)
```

```{r}
# Set variables of interest
vars <- c("pct_bb", "pct_college", "pct_foreign_born", "pct_it_workers", "median_hh_inc")

# Loop through variables and merge with train_df
merged_df <- train_df

for (var in vars) {
  # Select columns and pivot longer
  merged_df <- imputed_data %>%
    select(cfips, paste0(var, "_2017"):paste0(var, "_2020")) %>%
    pivot_longer(cols = starts_with(var),
                 names_to = "year",
                 values_to = var) %>%
    # Modify year and month columns
    mutate(year = as.integer(str_sub(year, -4)) + 2) %>%
    uncount(12, .id = "month") %>%
    mutate(month = month) %>%
    # Merge with merged_df
    merge(merged_df, by = c("cfips", "year", "month"), all.x = TRUE) %>%
    arrange(cfips, row_id) 
}
merged_df <- merged_df %>%
  select(row_id, cfips, county, state, first_day_of_month, microbusiness_density, active, year_month, year, month, pct_bb, pct_college, pct_foreign_born, pct_it_workers, median_hh_inc) %>%
  # remove NA values created because of the 
  na.omit(merged_df)
```

```{r}
colSums(is.na(merged_df))
```

```{r}
str(train_df)
cat(rep("=", 40), "\n")
str(test_df)
```

```{r fig.height=4}
# Create plots
p1 <- train_df %>%
  # Group train_df by year_month
  group_by(year_month) %>%
  # calculate the mean value of microbusiness_density for each group
  summarise(mean_microbusiness_density = mean(microbusiness_density)) %>%
  ggplot(aes(x = year_month, y = mean_microbusiness_density, group = 1)) +
  geom_line() +
  labs(title = "Overall Average Microbusiness Density",
       x = "Year-Month",
       y = "Average Microbusiness Density") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2 <- train_df %>%
  group_by(year_month) %>%
  summarize(avg_active = mean(active)) %>%
  ggplot(aes(x = year_month, y = avg_active)) +
  geom_line(group = 1) +
  labs(title = "Overall Active Microbusiness Count",
       x = "Year-Month", 
       y = "Active") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Display the plots
grid.arrange(p1, p2, nrow = 2)

```

```{r fig.height=3}
# Group train_df by year and calculate the mean value of microbusiness_density for each group
train_df_mean_year <- train_df %>%
  group_by(year) %>%
  summarize(avg_microbusiness_density = mean(microbusiness_density))

# Group train_df by month and calculate the mean value of the target variable for each group
train_df_mean_month <- train_df %>%
  group_by(month) %>%
  summarize(avg_microbusiness_density = mean(microbusiness_density))

# Plot the monthly mean values
p1 <- 
  ggplot(train_df_mean_month, aes(x = month, y = avg_microbusiness_density)) +
  geom_line() +
  ggtitle("Avg Monthly Microbusiness Density") +
  xlab("Month") +
  ylab("Average Microbusiness Density") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot the yearly mean values
p2 <- 
  ggplot(train_df_mean_year, aes(x = year, y = avg_microbusiness_density)) +
  geom_line() +
  ggtitle("Avg Yearly Microbusiness Density") +
  xlab("Year") +
  ylab("Average Microbusiness Density") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the plots side by side
grid.arrange(p1, p2, ncol=2)
```

The Bureau of Economic Analysis (BEA) divides the United States into eight distinct economic regions. These regions are based on similarities in economic characteristics such as industry composition, income levels, and employment patterns. The eight regions are:

1.  New England: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont.

    The economy in this region is largely based on manufacturing, healthcare, education, and finance.

2.  Mideast: Delaware, Maryland, New Jersey, New York, Pennsylvania, and the District of Columbia.

    The region has a diverse economy, with a mix of manufacturing, finance, healthcare, and professional services.

3.  Great Lakes: Illinois, Indiana, Michigan, Ohio, and Wisconsin.

    The region has a strong manufacturing base, particularly in the automotive industry, and also has a significant healthcare sector.

4.  Plains: Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, and South Dakota.

    Agriculture and energy production are major industries in this region, along with manufacturing and healthcare.

5.  Southeast: Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, and West Virginia.

    The Southeast has a diverse economy, with significant industries in healthcare, finance, and manufacturing, as well as tourism and agriculture.

6.  Southwest: Arizona, New Mexico, Oklahoma, and Texas.

    The region has a strong energy sector, particularly in oil and gas production, and also has significant industries in manufacturing, healthcare, and finance.

7.  Rocky Mountain: Colorado, Idaho, Montana, Utah, and Wyoming.

    The region is known for its natural resources, particularly in mining and energy production, as well as tourism, healthcare, and manufacturing.

8.  Far West: Alaska, California, Hawaii, Nevada, Oregon, and Washington.

    This region has a diverse economy, with significant industries in technology, finance, healthcare, and manufacturing, as well as tourism and agriculture.

```{r}
# Get the map of the United States
us_map <- map_data("state")

# Create a lookup table for state abbreviations and their corresponding full names
state_names <- data.frame(state = state.abb, name = tolower(state.name))

# Map the regions to the states
region_map <- us_map %>%
#left_join(state_names, by = c("region" = "state")) %>%
  left_join(state_names, by = c("region" = "name")) %>%
# merge(us_map, state_names, by.x=c("region"), by.y=c("name")) %>%
  mutate(region = 
           ifelse(region %in% c("connecticut", "maine", "massachusetts", "new hampshire", "rhode island", "vermont"), "New England",
                         ifelse(region %in% c("delaware", "maryland", "new jersey", "new york", "pennsylvania", "district of columbia"), "Mideast",
                                ifelse(region %in% c("illinois", "indiana", "michigan", "ohio", "wisconsin"), "Great Lakes",
                                       ifelse(region %in% c("iowa", "kansas", "minnesota", "missouri", "nebraska", "north dakota", "south dakota"), "Plains",
                                              ifelse(region %in% c("alabama", "arkansas", "florida", "georgia", "kentucky", "louisiana", "mississippi", "north carolina", "south carolina", "tennessee", "virginia", "west virginia"), "Southeast",
                                                     ifelse(region %in% c("arizona", "new mexico", "oklahoma", "texas"), "Southwest",
                                                            ifelse(region %in% c("colorado", "idaho", "montana", "utah", "wyoming"), "Rocky Mountain",
                                                                   ifelse(region %in% c("alaska", "california", "hawaii", "nevada", "oregon", "washington"), "Far West", NA
                                                                          )
                                                                   )
                                                            )
                                                     )
                                              )
                                       )
                                )
                         )
         )

# Summarize the data to get the center coordinates of each state
#state_centers <- region_map %>% 
#  group_by(state) %>% 
#  summarise(long = mean(long), lat = mean(lat))
# add labels
states <- aggregate(cbind(long, lat) ~ region, data=us_map, 
                FUN=function(x)mean(range(x)))
states$group <- c("AL", "AR", "AZ", "CA", "CO", "CT", "DE", "DC", "FL", "GA", "IA", 
              "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", 
              "MO", "MS", "MT", "NC", "ND", "NE", "NH", "NJ", "NM", "NV", "NY", 
              "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VA", 
              "VT", "WA", "WI", "WV", "WY")

# names(states)[names(states) == "region"] <- "group"


#Plot the map
ggplot(region_map, aes(x = long, y = lat, group = group, fill = region)) +
  geom_polygon(color = "black", show.legend = TRUE) +
#  geom_text(aes(label = state), data = region_map, size = 3, vjust = 2, hjust = 2) +
#  geom_text(aes(label = state), data = state_centers, size = 2, vjust = 2, hjust = 2) +
  geom_text(data = states, aes(long, lat, label = group), size = 2.5, inherit.aes = FALSE, color = "white", fontface = "bold") +
#  scale_fill_gradient(low = "white", high = "darkred") +
#  scale_fill_manual(values = viridis(n = 60), na.value = "gray") +
  labs(title = "Bureau of Economic Analysis Regional Divisions Map", fill = "Region") +
#  geom_text(aes(x = long, y = lat, label = state), data = state_centers, size = 3, color = "white") +
  theme_void() +
  theme(panel.background = element_rect(fill = "gray75", color = NA))


```

```{r}
str(states)
```

First, we need to convert state and county columns in `train_df` to lowercase letters. Because, the data from map_data() will be in lowercase and when merging two dataframes it might cause problems.

```{r}
# Convert state and county columns in train_df to lowercase
train_df <- train_df %>% 
  mutate(state = tolower(state)) %>%
  mutate(county = tolower(county))

```

Then, we'll create a new column named region and assign region values based on state column:

```{r}
# Create a new column named region and initialize all values as NA
train_df$region <- NA

# Assign region values based on state column
for (i in 1:nrow(train_df)) {
  if (train_df$state[i] %in% c("connecticut", "maine", "massachusetts", "new hampshire", "rhode island", "vermont")) {
    train_df$region[i] <- "new england"
  } else if (train_df$state[i] %in% c("delaware", "maryland", "new jersey", "new york", "pennsylvania", "district of columbia")) {
    train_df$region[i] <- "mideast"
  } else if (train_df$state[i] %in% c("illinois", "indiana", "michigan", "ohio", "wisconsin")) {
    train_df$region[i] <- "great lakes"
  } else if (train_df$state[i] %in% c("iowa", "kansas", "minnesota", "missouri", "nebraska", "north dakota", "south dakota")) {
    train_df$region[i] <- "plains"
  } else if (train_df$state[i] %in% c("alabama", "arkansas", "florida", "georgia", "kentucky", "louisiana", "mississippi", "north carolina", "south carolina", "tennessee", "virginia", "west virginia")) {
    train_df$region[i] <- "southeast"
  } else if (train_df$state[i] %in% c("arizona", "new mexico", "oklahoma", "texas")) {
    train_df$region[i] <- "southwest"
  } else if (train_df$state[i] %in% c("colorado", "idaho", "montana", "utah", "wyoming")) {
    train_df$region[i] <- "rocky mountain"
  } else if (train_df$state[i] %in% c("alaska", "california", "hawaii", "nevada", "oregon", "washington")) {
    train_df$region[i] <- "far west"
  } else {
    train_df$region[i] <- "other"
  }
}

```

```{r}
# Print all the unique values in the region column
unique(train_df$region)
```

```{r}
# Group train_df by region and calculate average microbusiness density
train_df %>%
  group_by(region) %>%
  summarize(avg_density = mean(microbusiness_density)) %>%

  # Create bar plot of average density by region
  ggplot(aes(x = region, y = avg_density)) +
  geom_bar(stat = "identity") +
  
  # Add plot title and axis labels
  labs(title = "Average Microbusiness Density Per Region",
       x = "Region", y = "Avg Density") +
  
  # Apply a black and white theme to the plot
  theme_bw()
```

```{r}
# Group train_df by region and calculate average microbusiness density
avg_density <- train_df %>%
  group_by(region) %>%
  summarize(avg_density = mean(microbusiness_density))

# Create a lookup table for state abbreviations and their corresponding full names
state_names <- data.frame(state = state.abb, name = tolower(state.name))

# Lowercase region column of region_map
region_map <- region_map %>% 
  mutate(region = tolower(region)) 

# Merge the average density data with the region_map data
plot_data <- merge(region_map, avg_density, by = "region") %>%
  arrange(order)

# Coordinates of the center of regions
bea_regions <- data.frame(
  group = c("New England", "Mideast", "Great Lakes", "Plains", 
             "Southeast", "Southwest", "Rocky Mountain", "Far West"),
  x = c(-71.8, -76.9, -86.6, -98.5, -82.4, -106.4, -111.1, -119.8),
  y = c(42.2, 39, 43.4, 39.8, 32.6, 34.3, 44.4, 38.4)
)

# Create the plot
ggplot(plot_data, aes(x = long, y = lat, group = group, fill = avg_density)) +
  geom_polygon(color = "black") +
  geom_label(data = bea_regions,
             aes(x = x, y = y, label = group),
             size = 3, fontface = "bold", 
             label.padding = unit(0.2, "lines"),
             label.size = 0.2,
             fill = "gray75", color = "black") +
  scale_fill_gradient(low = "white", high = "darkred") +
  # scale_fill_viridis(name = "Avg Density", na.value = "gray") +
  labs(title = "Average Microbusiness Density Per Region", fill = "Avg Density") +
  theme_void()

```

```{r}
# Aggregate microbusiness density by state
state_avg <- aggregate(microbusiness_density ~ state, data = train_df, FUN = mean)

# Load US map data
us_map <- map_data("state")

# Merge state_avg with us_map based on region and state
map_data <- merge(us_map, state_avg, by.x = "region", by.y = "state")

# Create a heatmap of microbusiness density by state
ggplot(map_data, aes(x = long, y = lat, group = group, fill = microbusiness_density)) +
  geom_polygon() +
  scale_fill_gradient(low = "white", high = "navyblue") +
  coord_map() +
  labs(title = "Average Microbusiness Density per State", fill = "Density") +
  theme_void() +
  theme(panel.background = element_rect(fill = "lightblue", color = NA))
```

```{r}
#{r fig.width = 10 ,fig.height = 12, out.width='100%', fig.align='center'}
# Aggregate microbusiness density by county

#county_avg <- train_df %>%
#  group_by(cfips, county) %>%
#  summarise(microbusiness_density = mean(microbusiness_density))

county_avg <- aggregate(microbusiness_density ~ county + state, data = train_df, FUN = mean)
county_avg$county <- gsub(" county", "", county_avg$county)
county_avg$county <- gsub(" city", "", county_avg$county)
county_avg$county <- gsub(" parish", "", county_avg$county)


# Load US county map data
us_map <- map_data("county")

# Merge county_avg with us_map based on region and county
map_data <- merge(us_map, county_avg, by.x = c("subregion", "region"), by.y = c("county", "state")) %>%
  arrange(order)

# Create a heatmap of microbusiness density by county using ggplot2
ggplot(map_data, aes(x = long, y = lat, group = group, fill = microbusiness_density)) +
  geom_polygon() +
  scale_fill_gradient(low = "lightblue", high = "navyblue") +
  coord_map() +
  labs(title = "Average Microbusiness Density per County", fill = "Density") +
  theme_void() +
  theme(panel.background = element_rect(fill = "gray85", color = NA))

```

```{r}
str(merged_df)
```

Boxplots are a visualization tool that provide insights into the central tendency and spread of a dataset, as well as identify outliers and skewness. They are useful for detecting anomalies and comparing variable distributions in a dataset, providing valuable insights into data distribution for exploratory data analysis.

```{r fig.height=4, fig.width=3}
boxplot(merged_df$microbusiness_density, col = "blue", main = "Microbusiness Density")
```

```{r fig.height=10, fig.width=4}
par(mfrow=c(3,2)) # set plot layout to 3 rows and 2 columns
boxplot(merged_df$median_hh_inc, col = "pink", main = "Median Household Income")
boxplot(merged_df$pct_college, col = "pink", main = "Percentage with College Education")
boxplot(merged_df$pct_foreign_born, col = "pink", main = "Percentage of Foreign-born Residents")
boxplot(merged_df$pct_it_workers, col = "pink", main = "Percentage of IT Workers")
boxplot(merged_df$pct_bb, color = "pink", main = "Percentage of Broadband Access")
boxplot(merged_df$active, color = "pink", main = "Active Microbusiness Count")

```

### Outlier detection:

Outlier detection is an important step in data analysis, as outliers can significantly affect the results of statistical analyses. One method for outlier detection is the decision range approach.

The decision range approach involves setting a range of values outside of which any observations are considered outliers. The decision range is determined based on the data distribution and the researcher's judgment. One common approach is to use the interquartile range (IQR) to define the decision range. The IQR is calculated as the difference between the third quartile (Q3) and the first quartile (Q1) of the data.

The decision range is then defined as the range from Q1 - 1.5*IQR to Q3 + 1.5*IQR. Any observations that fall outside of this range are considered outliers. This method is useful for identifying potential outliers in a dataset and can help to ensure that statistical analyses are robust and accurate.

```{r}
quartiles <- quantile(merged_df$microbusiness_density, probs = seq(0, 1, 0.25), na.rm = FALSE,
         names = TRUE, type = 7, digits = 6)
quartiles
```

```{r fig.width=12, fig.height=4}
# Calculate the mean and standard deviation of 'microbusiness_density'
mean_density <- mean(merged_df$microbusiness_density)
sd_density <- sd(merged_df$microbusiness_density)

# Create a range of values for the x-axis
x_values <- seq(mean_density - 3*sd_density, mean_density + 3*sd_density, length.out = 1000)

# Create a bell curve using the 'dnorm' function with mean and standard deviation calculated above
y_values <- dnorm(x_values, mean = mean_density, sd = sd_density)

# Combine the 'x_values' and 'y_values' into a data frame using the 'data.frame' function
density_df <- data.frame(x = x_values, y = y_values)

# Create a boxplot using the 'ggplot' function from 'ggplot2' package
boxplot <- ggplot(data = merged_df, aes(x = "", y = merged_df$microbusiness_density)) +
  geom_boxplot(fill = "skyblue") +
  labs(x = "", y = "Microbusiness Density") +
  ggtitle("Boxplot for Microbusiness Density")

# Create a Q-Q plot using the 'ggplot' function from 'ggplot2' package and add a diagonal line using the 'geom_abline' function
qqplot <- ggplot(data = density_df, aes(sample = x)) +
  geom_qq() +
  geom_abline(intercept = 0, slope = 1, colour = "red") +
  labs(x = "Theoretical Normal Quantiles", y = "Observed Quantiles") +
  ggtitle("Q-Q Plot for Microbusiness Density")

# Create a bell curve using the 'ggplot' function from 'ggplot2' package and add the bell curve using the 'geom_line' function
densityplot <- ggplot(data = merged_df, aes(x = microbusiness_density)) +
  geom_histogram(aes(y = ..density..), bins = 30, colour = "black", fill = "white") +
  geom_line(data = density_df, aes(x = x, y = y), colour = "red", size = 1) +
  labs(x = "Microbusiness Density", y = "Density") +
  ggtitle("Distribution of Microbusiness Density")

# Arrange the plots in one row using the 'grid.arrange' function from the 'gridExtra' package
grid.arrange(densityplot, boxplot, qqplot, ncol = 3, widths = c(2, 1, 2))

```

```{r}
par(mfrow=c(1,3)) # set plot layout to 3 rows and 2 columns
mean_density <- mean(merged_df$microbusiness_density)
sd_density <- sd(merged_df$microbusiness_density)
x_values <- seq(mean_density - 3*sd_density, mean_density + 3*sd_density, length.out = 1000)
y_values <- dnorm(x_values, mean = mean_density, sd = sd_density)
density_df <- data.frame(x = x_values, y = y_values)
ggplot(merged_df, aes(x = microbusiness_density)) +
  geom_histogram(aes(y = ..density..), bins = 30, colour = "black", fill = "lightblue") +
  geom_line(data = density_df, aes(x = x, y = y), colour = "navy", linewidth = 1) +
  labs(x = "Microbusiness Density", y = "Density") +
  ggtitle("Distribution of Microbusiness Density") +
  theme_minimal()

theoretical_norm <- qnorm(p = seq(0, 1, length.out = length(merged_df$microbusiness_density)), mean = mean(merged_df$microbusiness_density), sd = sd(merged_df$microbusiness_density))
density_df <- data.frame(observed = sort(merged_df$microbusiness_density), theoretical = theoretical_norm)
ggplot(density_df, aes(x = theoretical, y = observed)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, colour = "red") +
  labs(x = "Theoretical Normal Quantiles", y = "Observed Quantiles") +
  ggtitle("Q-Q Plot for Microbusiness Density")

ggplot(data = merged_df, aes(x = "", y = microbusiness_density)) +
  geom_boxplot(fill = "skyblue") +
  labs(x = "", y = "Microbusiness Density") +
  ggtitle("Boxplot for Microbusiness Density")

```

```{r}
# compute the correlation matrix
corr_matrix <- cor(imputed_data)

# visualize the correlation matrix using corrplot
corrplot(corr_matrix, type = "upper", method = "circle")
```

```{r}
library(ggplot2)
library(reshape2)

# Calculate the correlation matrix
cor_mat <- cor(imputed_data)

# Melt the correlation matrix to a long format
melted_cor <- melt(cor_mat)

# Generate the heatmap
ggplot(melted_cor, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 8, hjust = 1),
        axis.text.y = element_text(size = 8),
        axis.title = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.justification = c(1, 0),
        legend.position = c(0.8, 0.2),
        legend.direction = "horizontal") 

```

```{r}
str(imputed_data)
```

```{r}
# Bar Plot
ggplot(imputed_data, aes(x=cfips, y=median_hh_inc_2017)) +
  geom_bar(stat="identity", fill="steelblue") +
  labs(title = "Median Household Income in 2017",
       x = "County FIPS Code", y = "Median Household Income")

```

```{r}
# Pie Chart
ggplot(imputed_data, aes(x="", y=pct_foreign_born_2017, fill=cfips)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start=0) +
  labs(title = "Percentage of Foreign-born Population in 2017",
       fill = "County FIPS Code")

```

```{r fig.width=4.5 ,fig.height = 4,out.width='100%', fig.align='center'}
#creating a boxplot of the 'first_interval' column of the CustomerChurn_temp dataframe, grouped by the 'first_interval_label' column.
ggplot(merged_df, aes("", microbusiness_density)) +
  geom_boxplot()+scale_fill_manual(values = c("0" = "#009999","1" = "#CC3300"))
```

```{r}
#Outlier Detection using decision range:
#Any data point lies outside of this range is considered as outliers
#We have lower bound and upper bound. any data outside of these 2 bounds will be considered as outliers. 
#Here we are just going to consider to consider the upper bound, as we can't able to see any outliers in lower bound region.

#Upper Bound: (Q3 + 1.5 * IQR)

#Finding the Quartiles(2nd Quartile and 3rd quartile)
quartiles <- quantile(train_df$microbusiness_density, probs=c(.25, .75), na.rm = FALSE)
#quartiles

#getting the IQR
iqr<-IQR(train_df$microbusiness_density)
#upper bound
second_interval_ub <- quartiles[2] + 1.5*iqr 
second_interval_ub

#now we can label the second_interval based on upper bound
train_df$microbusiness_density_label<-as.numeric(train_df$microbusiness_density > second_interval_ub)



```

```{r}
# Create the plot
ggplot(merged_df, aes(x = "", y = microbusiness_density)) +
  geom_boxplot(fill = "lightblue") +
  #labs(title = "doesn;t matter",
  #     x = NULL, y = NULL) +
  theme_minimal()
```

```{r}
library(ggplot2)

# Create the plot
ggplot(imputed_data, aes(x = "", y = pct_bb_2021)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Percentage of Population with Broadband Access in 2021",
       x = NULL, y = "Percentage") +
  theme_minimal()

```

```{r}
# Create the box plot
ggplot(imputed_data, aes(x = "", y = pct_bb_2017)) +
  geom_boxplot(fill = "lightblue", color = "blue") +
  labs(title = "Percentage of Broadband Access in 2017", y = "Percentage") +
  theme_bw()

```

```{r}
# Scatter Plot
ggplot(imputed_data, aes(x=median_hh_inc_2017, y=pct_it_workers_2017)) +
  geom_point() +
  labs(title = "Percentage of IT Workers and Median Household Income in 2017",
       x = "Median Household Income", y = "Percentage of IT Workers")

```

```{r}
# Heatmap
imputed_data_long <- melt(imputed_data, id.vars="cfips", 
                          measure.vars=c("pct_bb_2017", "pct_bb_2018", "pct_bb_2019", 
                                         "pct_bb_2020", "pct_bb_2021"), 
                          variable.name="year", value.name="pct_bb")

ggplot(imputed_data_long, aes(x=cfips, y=year, fill=pct_bb)) +
  geom_tile() +
  scale_fill_gradient(low="white", high="steelblue") +
  labs(title = "Percentage of Broadband Coverage",
       x = "County FIPS Code", y = "Year")

```

```{r}

us.map <-  map_data('state')

# add PADD zones
us.map$PADD[us.map$region %in% 
          c("maine", "vermont", "new hampshire", "massachusetts", "connecticut", "rhode island",
            "new york", "pennsylvania", "new jersey", "delaware", "district of columbia", "maryland",
            "west virginia", "virginia", "north carolina", "south carolina", "georgia", "florida")] <- "PADD 1: East Coast"
us.map$PADD[us.map$region %in% 
          c("south dakota", "north dakota","nebraska", "kansas", "oklahoma", 
            "minnesota", "iowa", "missouri", "wisconsin", "illinois", "indiana",
            "michigan", "ohio", "kentucky", "tennessee")] <- "PADD 2: Midwest"
us.map$PADD[us.map$region %in% 
          c("new mexico", "texas", "arkansas", "louisiana", "alabama", "mississippi")] <- "PADD 3: Gulf Coast"
us.map$PADD[us.map$region %in% 
          c("montana", "idaho", "wyoming", "utah", "colorado")] <- "PADD 4: Rocky Mountain"
us.map$PADD[us.map$region %in% 
          c("washington", "oregon", "nevada", "arizona", "california")] <- "PADD 5: West Coast"

# subset the dataframe by padd zones and move lat/lon accordingly
us.map$lat.transp[us.map$PADD == "PADD 1: East Coast"] <- us.map$lat[us.map$PADD == "PADD 1: East Coast"]
us.map$long.transp[us.map$PADD == "PADD 1: East Coast"] <- us.map$long[us.map$PADD == "PADD 1: East Coast"] + 5

us.map$lat.transp[us.map$PADD == "PADD 2: Midwest"] <- us.map$lat[us.map$PADD == "PADD 2: Midwest"]
us.map$long.transp[us.map$PADD == "PADD 2: Midwest"] <- us.map$long[us.map$PADD == "PADD 2: Midwest"]

us.map$lat.transp[us.map$PADD == "PADD 3: Gulf Coast"] <- us.map$lat[us.map$PADD == "PADD 3: Gulf Coast"] - 3
us.map$long.transp[us.map$PADD == "PADD 3: Gulf Coast"] <- us.map$long[us.map$PADD == "PADD 3: Gulf Coast"]

us.map$lat.transp[us.map$PADD == "PADD 4: Rocky Mountain"] <- us.map$lat[us.map$PADD == "PADD 4: Rocky Mountain"]
us.map$long.transp[us.map$PADD == "PADD 4: Rocky Mountain"] <- us.map$long[us.map$PADD == "PADD 4: Rocky Mountain"] - 5

us.map$lat.transp[us.map$PADD == "PADD 5: West Coast"] <- us.map$lat[us.map$PADD == "PADD 5: West Coast"] - 2
us.map$long.transp[us.map$PADD == "PADD 5: West Coast"] <- us.map$long[us.map$PADD == "PADD 5: West Coast"] - 10

# add labels
states <- aggregate(cbind(long.transp, lat.transp) ~ region, data=us.map, 
                FUN=function(x)mean(range(x)))
states$labels <- c("AL", "AR", "AZ", "CA", "CO", "CT", "DE", "DC", "FL", "GA", "IA", 
              "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", 
              "MO", "MS", "MT", "NC", "ND", "NE", "NH", "NJ", "NM", "NV", "NY", 
              "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VA", 
              "VT", "WA", "WI", "WV", "WY")



# plot and use padd zone as fill
ggplot(us.map,  aes(x=long.transp, y=lat.transp), colour="white") + 
  geom_polygon(aes(group = group, fill=PADD)) +
  geom_text(data=states, aes(long.transp, lat.transp, label=labels), size=3) +
  theme(panel.background = element_blank(),  # remove background
    panel.grid = element_blank(), 
    axis.line = element_blank(), 
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    axis.text = element_blank()) +
  coord_equal()
```

```{r}
# Create a new column named "subregion" and initialize all values as NA
us_map$subregion <- NA

# Assign subregion values based on region column
for (i in 1:nrow(us_map)) {
  if (us_map$region[i] %in% c("connecticut", "maine", "massachusetts", "new hampshire", "rhode island", "vermont")) {
    us_map$subregion[i] <- "new england"
  } else if (us_map$region[i] %in% c("delaware", "maryland", "new jersey", "new york", "pennsylvania", "district of columbia")) {
    us_map$subregion[i] <- "mideast"
  } else if (us_map$region[i] %in% c("illinois", "indiana", "michigan", "ohio", "wisconsin")) {
    us_map$subregion[i] <- "great lakes"
  } else if (us_map$region[i] %in% c("iowa", "kansas", "minnesota", "missouri", "nebraska", "north dakota", "south dakota")) {
    us_map$subregion[i] <- "plains"
  } else if (us_map$region[i] %in% c("alabama", "arkansas", "florida", "georgia", "kentucky", "louisiana", "mississippi", "north carolina", "south carolina", "tennessee", "virginia", "west virginia")) {
    us_map$subregion[i] <- "southeast"
  } else if (us_map$region[i] %in% c("arizona", "new mexico", "oklahoma", "texas")) {
    us_map$subregion[i] <- "southwest"
  } else if (us_map$region[i] %in% c("colorado", "idaho", "montana", "utah", "wyoming")) {
    us_map$subregion[i] <- "rocky Mountain"
  } else if (us_map$region[i] %in% c("alaska", "california", "hawaii", "nevada", "oregon", "washington")) {
    us_map$subregion[i] <- "far west"
  } else {
    us_map$subregion[i] <- "other"
  }
}


```

```{r}


```
