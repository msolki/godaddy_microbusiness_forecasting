---
title: "GoDaddy - Microbusiness Density Forecasting"
author: "Mohammad Solki"
date: "2023-02-28"
output:
  word_document: default
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: inline
---

# 1. Introduction

## 1.1. Goal of the Competition

The challenge in this competition is to forecast microbusiness activity across the United States, as measured by the density of microbusinesses in US counties. Microbusinesses are often too small or too new to show up in traditional economic data sources, but microbusiness activity may be correlated with other economic indicators of general interest.

This work will help policymakers gain visibility into microbusinesses, a growing trend of very small entities. Additional information will enable new policies and programs to improve the success and impact of these smallest of businesses.

GoDaddy's Venture Forward team has gathered data on over 20 million microbusinesses in the United States, defined as businesses with an online presence and ten or fewer employees, to help policymakers understand the factors associated with these small businesses. While traditional economic data sources often miss these businesses, GoDaddy's survey data can provide insights into this sector of the economy. The data can be used to improve predictions and inform decision-making to create more inclusive and resilient economies. The competition hosted by GoDaddy aims to empower entrepreneurs by giving them the tools they need to grow online and make a substantial impact on communities across the country.

Model accuracy will be evaluated on SMAPE (Symmetric mean absolute percentage error) between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.

SMAPE formula is usually defined as follows:

$$
\text{SMAPE} = \frac{100\%}{n} \sum_{t=1}^{n} \frac{\left\lvert F_t - A_t \right\rvert}{( \left\lvert F_t \right\rvert + \left\lvert A_t \right\rvert)/2}
$$

where:

-   $n$ is the number of observations in the time series

-   $F_t$ is the forecasted value at time $t$

-   $A_t$ is the actual value at time $t$

-   $\left\lvert x \right\rvert$ denotes the absolute value of $x$.

## 1.2. Datasets

A great deal of data is publicly available about counties and we have not attempted to gather it all here. You are strongly encouraged to use external data sources for features.

**train.csv**

-   `row_id` An ID code for the row.

-   `cfips` A unique identifier for each county using the Federal Information Processing System. The first two digits correspond to the state FIPS code, while the following 3 represent the county.

-   `county_name` The written name of the county.

-   `state_name` The name of the state.

-   `first_day_of_month` The date of the first day of the month.

-   `microbusiness_density` Microbusinesses per 100 people over the age of 18 in the given county. This is the target variable. The population figures used to calculate the density are on a two-year lag due to the pace of update provided by the U.S. Census Bureau, which provides the underlying population data annually. 2021 density figures are calculated using 2019 population figures, etc.

-   `active` The raw count of microbusinesses in the county. Not provided for the test set.

**test.csv** Metadata for the submission rows. This file will remain unchanged throughout the competition.

-   `row_id` An ID code for the row.

-   `cfips` A unique identifier for each county using the Federal Information Processing System. The first two digits correspond to the state FIPS code, while the following 3 represent the county.

-   `first_day_of_month` The date of the first day of the month.

**census_starter.csv** Examples of useful columns from the Census Bureau's American Community Survey (ACS) atÂ [data.census.gov](https://data.census.gov/). The percentage fields were derived from the raw counts provided by the ACS. All fields have a two year lag to match what information was avaiable at the time a given microbusiness data update was published.

-   `pct_bb_[year]` The percentage of households in the county with access to broadband of any type. Derived from ACS table B28002: PRESENCE AND TYPES OF INTERNET SUBSCRIPTIONS IN HOUSEHOLD.

-   `cfips` The CFIPS code.

-   `pct_college_[year]` The percent of the population in the county over age 25 with a 4-year college degree. Derived from ACS table S1501: EDUCATIONAL ATTAINMENT.

-   `pct_foreign_born_[year]` The percent of the population in the county born outside of the United States. Derived from ACS table DP02: SELECTED SOCIAL CHARACTERISTICS IN THE UNITED STATES.

-   `pct_it_workers_[year]` The percent of the workforce in the county employed in information related industries. Derived from ACS table S2405: INDUSTRY BY OCCUPATION FOR THE CIVILIAN EMPLOYED POPULATION 16 YEARS AND OVER.

-   `median_hh_inc_[year]` The median household income in the county. Derived from ACS table S1901: INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS).

# 2. Setup the Environment

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  out.width = '100%',
  fig.align = 'center'
  )
```

First, we'll set the working directory using **`setwd()`**, and then import the required libraries. As we proceed through the report the list of libraries might change.

```{r}
# Set the working directory
setwd("/Users/dreamer/Downloads/Godaddy/godaddy_microbusiness_forecasting")
```

```{r message = FALSE}
# Importing the libraries

# Recognize package conflicts
library(conflicted)

# Multi-purpose package for data import, tidying, manipulation, visualisation, and programming
# Most common packages include: ggplot2, purrr, tibble, dplyr, tidyr, stringr, readr, forcats
library(tidyverse)

# Deal with missing data
library(mice)

# Related to plots
library(maps)
#library(ggmap)
library(gridExtra)
library(mapdata)
library(ggcorrplot)
library(corrplot)

# Training
library(caret)
library(gbm)

# Color palette
library(viridis)

# Future Selection 
#library(KernSmooth)
library(glmnet)
library(randomForest)
library(rpart)

# libraries required for calculating SMAPE
library(forecast)
library(Metrics)
```

# 3. Exploratory Data Analysis

## 3.1. Exploring the datasets

Explore the datasets to get a better understanding of the data.\
Load the **train**, **test**, and **census_starter** datasets into R dataframes.

```{r}
# Load train.csv into a dataframe
train_df <- read.csv("./datasets/train.csv")

# Load test.csv into a dataframe
test_df <- read.csv("./datasets/test.csv")

# Load census_starter.csv into a dataframe
census_df <- read.csv("./datasets/census_starter.csv")

```

After reading the CSV files into dataframes, we should check whether the data is loaded correctly or not. We can use the `head()` function of R to display the first few rows of the dataframes and `tail()` function to display the last rows. This will display the first and last six rows of the **train**, **test** and **census** dataframes. We can also use other R functions such as `str()` and `summary()` to get more information about the dataframes, such as column names, data types, and summary statistics.

```{r}
# Display the first 6 rows of the dataframes
head(train_df)
head(test_df)
head(census_df)

```

```{r}
# Display the last 6 rows of the dataframes
tail(train_df)
tail(test_df)
tail(census_df)
```

```{r}
# Display information about the train dataframe
str(train_df)
#cat(rep("=", 40), "\n") # Print a line of 40 equal signs
summary(train_df)
```

```{r}
# Display information about the test dataframe
str(test_df)
#cat(rep("=", 40), "\n") # Print a line of 40 equal signs
summary(test_df)
```

```{r}
# Display information about the census dataframe
str(census_df)
#cat(rep("=", 40), "\n") # Print a line of 40 equal signs
summary(census_df)
```

The data type of `first_day_of_month` column in **train_df** and **test_df** is *character*. We will convert the character to *Date* format.

```{r}
# Change first_day_of_month data type to Date
train_df$first_day_of_month <- as.Date(train_df$first_day_of_month)
test_df$first_day_of_month <- as.Date(test_df$first_day_of_month)
str(train_df$first_day_of_month)
```

## 3.2. Checking the Dataframes for Missing Values

The `is.na()` function is used to create a logical matrix where *TRUE* represents a missing value and *FALSE* represents a non-missing value. The `colSums()` function is then used to count the number of missing values in each column of the data frame. If the sum of a column is greater than 0, it means that there is at least one missing value in that column.

```{r}
# Check for missing values in the train data frame
colSums(is.na(train_df))
```

```{r}
# Check for missing values in the test data frame
colSums(is.na(test_df))
```

```{r}
# Check for missing values in the census data frame
colSums(is.na(census_df))
```

```{r fig.width=6}
#{r fig.width=7, fig.align='center', fig.height=4, out.width='100%'}
# Calculate the number and percentage of missing values for each column
missing_data <- census_df %>%
  summarise_all(~ sum(is.na(.))) %>%
  gather(variable, missing_count) %>%
  mutate(missing_percent = missing_count/nrow(census_df)*100)

# Create two plots side by side
plot1 <- ggplot(missing_data, aes(x = missing_count, y = variable)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Number of missing values", y = "") +
  ggtitle("Number of missing values in census_df") +
  theme_gray()

plot2 <- ggplot(missing_data, aes(x = missing_percent, y = variable)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(x = "Percentage of missing values", y = "") +
  ggtitle("Percentage of missing values in census_df") +
  theme_gray()

# Arrange the two plots side by side
grid.arrange(plot1, plot2, ncol = 2)
```

We use the `complete.cases()` function to determine which rows have complete data and which rows have missing values. This function returns a logical vector indicating which rows have no missing values. Therefore, to identify the rows with missing values, we use the `!` operator to negate the logical vector returned by `complete.cases()`. Then, we use the `is.na()` function to identify which columns have missing values for each missing row:

```{r}
# Identify rows with missing values in census_df
missing_rows <- which(!complete.cases(census_df))

# Identify columns with missing values for each missing row
for (i in missing_rows) {
  cat("Row", i, "has missing values in columns:",
      paste(names(census_df)[is.na(census_df[i,])], collapse = ", "), "\n")
}
```

```{r}
print(census_df[missing_rows,])
```

## 3.3. Dealing with Missing Values

The **`mice`** package implements a method to deal with missing data. The package creates multiple imputations (replacement values) for multivariate missing data.

We'll use the `mice` package to impute missing values in the **census_df** dataframe with below arguments:

-   *m*: The number of imputations to generate was set to 5, because, generally, *m* should be set to at least 5 for good imputation performance. Creating too many datasets will increase the computational load and may not necessarily lead to better results.

-   *maxit*: The *maxit* value was set to 50 to allow for a larger number of iterations to ensure that the imputation algorithm converges and fills in missing values as accurately as possible.

-   *method*: In this case, we are using *"pmm"* which stands for *Predictive Mean Matching*, because it is a flexible and widely used imputation method that works well with continuous variables. The method estimates the missing values by drawing from a set of observed values that have similar characteristics to the missing values.

-   *print*: The print value is set to *FALSE* because this function prints a huge log output to console.

```{r}
# Impute missing data using mice
imputed_data <- mice(census_df, m = 5, maxit = 50, method = "pmm", print = FALSE)
# Extract imputed data
imputed_data <- complete(imputed_data)
```

```{r}
# Check for missing values in imputed data
colSums(is.na(imputed_data))

# Check the filled missing values 
print(imputed_data[missing_rows,])
```

## 3.4. Checking the Time Frame of *train* and *test* Dataframes

After dealing with the missing values, we have to check the time frame provided in the **train** and **test** datasets.

```{r}
index <- unique(train_df$first_day_of_month)
print(index)
```

The training data time frame includes 08/2019 to 10/2022

```{r}
index <- unique(test_df$first_day_of_month)
print(index)
```

The prediction dates provided include 11/2022 to 06/2023

## 3.5. Adding New Columns to *train* and *test*

To make analysis easier and be able to group the data by year and month, we will use **`substr()`** function to extract the relevant characters of the `first_day_of_month` column, which is a string that contains the date in the format "YYYY-MM-DD". Then, **`as.integer()`** function is used to convert the extracted year and month values from character strings to integers.

```{r}
# Add year, month and year_month columns to train_df
train_df$year <- as.integer(substr(train_df$first_day_of_month, 1, 4))
train_df$month <- as.integer(substr(train_df$first_day_of_month, 6, 7))
train_df$year_month <- substr(train_df$first_day_of_month, 1, 7)
str(train_df)
# Add year, month and year_month columns to test_df
test_df$year <- as.integer(substr(test_df$first_day_of_month, 1, 4))
test_df$month <- as.integer(substr(test_df$first_day_of_month, 6, 7))
test_df$year_month <- substr(test_df$first_day_of_month, 1, 7)
```

## 3.6. Merging the *train* and *imputed_data* datasets

The merging process is challenging because all data fields provided in the **imputed_data (**formerly **census_df)** dataframe have a two-year lag to match the data in the **train_df** dataframe. Also, the data provided in the **imputed_data** is on a yearly basis, but the data in the **train_df** dataframe is on a monthly basis. To merge these two dataframes, it is assumed that the yearly data provided is valid for all the months of the corresponding year. For example, data provided in the `pct_bb_2017` is valid for all the months of *2019* in the **train_df**.

```{r}
# Set variables of interest
vars <- c("pct_bb", "pct_college", "pct_foreign_born", "pct_it_workers", "median_hh_inc")

# Loop through variables and merge with train_df
merged_df <- train_df

for (var in vars) {
  # Select columns and pivot longer
  merged_df <- imputed_data %>%
    select(cfips, paste0(var, "_2017"):paste0(var, "_2020")) %>%
    pivot_longer(cols = starts_with(var),
                 names_to = "year",
                 values_to = var) %>%
    # Modify year and month columns
    mutate(year = as.integer(str_sub(year, -4)) + 2) %>%
    uncount(12, .id = "month") %>%
    mutate(month = month) %>%
    # Merge with merged_df
    merge(merged_df, by = c("cfips", "year", "month"), all.x = TRUE) %>%
    arrange(cfips, row_id) 
}
merged_df <- merged_df %>%
  select(row_id, cfips, county, state, first_day_of_month, microbusiness_density, active, year_month, year, month, pct_bb, pct_college, pct_foreign_born, pct_it_workers, median_hh_inc)

merged_test <- test_df
for (var in vars) {
  # Select columns and pivot longer
  merged_test <- imputed_data %>%
    select(cfips, paste0(var, "_2020"):paste0(var, "_2021")) %>%
    pivot_longer(cols = starts_with(var),
                 names_to = "year",
                 values_to = var) %>%
    # Modify year and month columns
    mutate(year = as.integer(str_sub(year, -4)) + 2) %>%
    uncount(12, .id = "month") %>%
    mutate(month = month) %>%
    # Merge with merged_df
    merge(merged_test, by = c("cfips", "year", "month"), all.x = TRUE) %>%
    arrange(cfips, row_id) 
}
merged_test <- merged_test %>%
  select(row_id, cfips, first_day_of_month, year_month, year, month, pct_bb, pct_college, pct_foreign_born, pct_it_workers, median_hh_inc)
  
```

```{r}
colSums(is.na(merged_df))
colSums(is.na(merged_test))
```

Since the data from 1/2019 to 7/2019 and 11/2022 to 12/2022 is not available in **train_df** merging the data has created NA values in **merged_df** for those months. Now we have to remove the rows with missing values.

```{r}
# remove NA values created in merged_df
merged_df <- merged_df %>%
  na.omit(merged_df)

merged_test <- merged_test %>%
  na.omit(merged_test)
```

```{r}
colSums(is.na(merged_df))
colSums(is.na(merged_test))
```

```{r}
summary(merged_df)
```

## 3.7. Data Visualization

The main feature in this project is `microbusiness_density` provided in the **merged_df.** Also, the number of active microbusinesses is provided in the `active` column.

### 3.7.1. Overall Microbusiness Density and Count

First, we will plot overall microbusiness density and count of active microbusiness in the United States:

```{r fig.height=4}
# Create plots
p1 <- merged_df %>%
  # Group merged_df by year_month
  group_by(year_month) %>%
  # calculate the mean value of microbusiness_density for each group
  summarise(mean_microbusiness_density = mean(microbusiness_density)) %>%
  ggplot(aes(x = year_month, y = mean_microbusiness_density, group = 1)) +
  geom_line() +
  labs(title = "Overall Microbusiness Density Average",
       x = "Year-Month",
       y = "Average Microbusiness Density") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2 <- merged_df %>%
  group_by(year_month) %>%
  summarize(avg_active = mean(active)) %>%
  ggplot(aes(x = year_month, y = avg_active)) +
  geom_line(group = 1) +
  labs(title = "Overall Active Microbusiness Count",
       x = "Year-Month", 
       y = "Active") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Display the plots
grid.arrange(p1, p2, nrow = 2)

```

As expected, these two graphs show almost similar behavior. If we ignore the slight fluctuations of the two graphs, the general microbusiness density and count are growing over the whole time frame.

```{r fig.height=4}
# Create plots
p1 <- merged_df %>%
  # Group merged_df by year_month
  group_by(year_month) %>%
  # calculate the median value of microbusiness_density for each group
  summarise(median_microbusiness_density = median(microbusiness_density)) %>%
  ggplot(aes(x = year_month, y = median_microbusiness_density, group = 1)) +
  geom_line() +
  labs(title = "Overall Microbusiness Density Median",
       x = "Year-Month",
       y = "Microbusiness Density Median") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2 <- merged_df %>%
  group_by(year_month) %>%
  summarize(median_active = median(active)) %>%
  ggplot(aes(x = year_month, y = median_active)) +
  geom_line(group = 1) +
  labs(title = "Overall Active Microbusiness Count Median",
       x = "Year-Month", 
       y = "Active Median") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Display the plots
grid.arrange(p1, p2, nrow = 2)
```

Then, we will examine the behavior of these two variables (*microbusiness density* and *active*) while grouping the data by month and year:

```{r fig.height=3}
# Group merged_df by year and calculate the mean value of microbusiness_density and active for each group
merged_df_mean_year <- merged_df %>%
  group_by(year) %>%
  summarize(avg_microbusiness_density = mean(microbusiness_density),
            avg_active = mean(active))

# Group merged_df by month and calculate the mean value of the microbusiness_density for each group
merged_df_mean_month <- merged_df %>%
  group_by(month) %>%
  summarize(avg_microbusiness_density = mean(microbusiness_density),
            avg_active = mean(active))

# Plot the monthly mean values for microbusiness density
p1 <- 
  ggplot(merged_df_mean_month, aes(x = month, y = avg_microbusiness_density)) +
  geom_line() +
  ggtitle("Average Monthly Microbusiness Density") +
  xlab("Month") +
  ylab("Avg Microbusiness Density") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot the yearly mean values for microbusiness density
p2 <- 
  ggplot(merged_df_mean_year, aes(x = year, y = avg_microbusiness_density)) +
  geom_line() +
  ggtitle("Average Yearly Microbusiness Density") +
  xlab("Year") +
  ylab("Avg Microbusiness Density") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot the monthly mean values for active
p3 <- 
  ggplot(merged_df_mean_month, aes(x = month, y = avg_active)) +
  geom_line() +
  ggtitle("Average Monthly Active Microbusiness Count") +
  xlab("Month") +
  ylab("Avg Microbusiness Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot the yearly mean values for active
p4 <- 
  ggplot(merged_df_mean_year, aes(x = year, y = avg_active)) +
  geom_line() +
  ggtitle("Average Yearly Active Microbusiness Count") +
  xlab("Year") +
  ylab("Avg Microbusiness Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

# Display the plots side by side
grid.arrange(p2, p1, p4, p3, nrow = 2, ncol = 2)
```

The left plots show that the average microbusiness density has increased slightly over the years, starting at approximately 3.73 in 2019 and reaching 3.94 in 2022. On the other hand, the average active count has also increased, starting at approximately 6274 in 2019 and reaching 6679 in 2022. In comparison, the right plots show fluctuations in the monthly averages for both variables. Generally, it follows a slightly upward trend over the year, with some peak values observed in July and October for the microbusiness density and active count, respectively. These peak values may represent seasonal variations, indicating that microbusinesses are more active during certain months. Overall, the plot shows some correlation between the monthly average values of microbusiness_density and active count, indicating that common factors may influence both variables.

```{r}
# Create a grid of box plots
ggplot(merged_df, aes(x=year_month, y=microbusiness_density)) +
  geom_boxplot() +
  labs(x="Year-Month", y="Microbusiness Density") +
  ggtitle("Overall Microbusiness Density Average") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Above plot is not very informative because the small values are obscured by the larger ones. Therefore, using a logarithmic scale on the Y-axis can help to reduce this distortion and provide a more informative visualization of the data.

```{r}
# Create a grid of box plots
ggplot(merged_df, aes(x=year_month, y=microbusiness_density)) +
  geom_boxplot() +
  scale_y_log10() +
  labs(x="Year-Month", y="Microbusiness Density") +
  ggtitle("Overall Microbusiness Density Average") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### 3.7.2. Economic Regional Divisions

The Bureau of Economic Analysis (BEA) divides the United States into eight distinct economic regions[^1].

[^1]: <https://doi.org/10.1371/journal.pone.0256407.g001>

These regions are based on similarities in economic characteristics such as industry composition, income levels, and employment patterns. The eight regions are:

1.  **New England**: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont.

    *The economy in this region is largely based on manufacturing, healthcare, education, and finance.*

2.  **Mideast**: Delaware, Maryland, New Jersey, New York, Pennsylvania, and the District of Columbia.

    *The region has a diverse economy, with a mix of manufacturing, finance, healthcare, and professional services.*

3.  **Great Lakes**: Illinois, Indiana, Michigan, Ohio, and Wisconsin.

    *The region has a strong manufacturing base, particularly in the automotive industry, and also has a significant healthcare sector.*

4.  **Plains**: Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, and South Dakota.

    *Agriculture and energy production are major industries in this region, along with manufacturing and healthcare.*

5.  **Southeast**: Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, and West Virginia.

    *The Southeast has a diverse economy, with significant industries in healthcare, finance, and manufacturing, as well as tourism and agriculture.*

6.  **Southwest**: Arizona, New Mexico, Oklahoma, and Texas.

    *The region has a strong energy sector, particularly in oil and gas production, and also has significant industries in manufacturing, healthcare, and finance.*

7.  **Rocky Mountain**: Colorado, Idaho, Montana, Utah, and Wyoming.

    *The region is known for its natural resources, particularly in mining and energy production, as well as tourism, healthcare, and manufacturing.*

8.  **Far West**: Alaska, California, Hawaii, Nevada, Oregon, and Washington.

    *This region has a diverse economy, with significant industries in technology, finance, healthcare, and manufacturing, as well as tourism and agriculture.*

To draw the map for the BEA regions, first, we need to convert state and county columns in **merged_df** to lowercase letters. Merging two dataframes will cause problems because the data from **`map_data()`** will be in lowercase letters.

```{r}
# Convert state and county columns in merged_df to lowercase
merged_df <- merged_df %>% 
  mutate(state = tolower(state)) %>%
  mutate(county = tolower(county))

```

Then, we'll create a new column in **merged_df** named `region` and assign region values based on `state` column:

```{r}
# Create a new column named region and initialize all values as NA
merged_df$region <- NA

# Assign region values based on state column
for (i in 1:nrow(merged_df)) {
  if (merged_df$state[i] %in% c("connecticut", "maine", "massachusetts", "new hampshire", "rhode island", "vermont")) {
    merged_df$region[i] <- "new england"
  } else if (merged_df$state[i] %in% c("delaware", "maryland", "new jersey", "new york", "pennsylvania", "district of columbia")) {
    merged_df$region[i] <- "mideast"
  } else if (merged_df$state[i] %in% c("illinois", "indiana", "michigan", "ohio", "wisconsin")) {
    merged_df$region[i] <- "great lakes"
  } else if (merged_df$state[i] %in% c("iowa", "kansas", "minnesota", "missouri", "nebraska", "north dakota", "south dakota")) {
    merged_df$region[i] <- "plains"
  } else if (merged_df$state[i] %in% c("alabama", "arkansas", "florida", "georgia", "kentucky", "louisiana", "mississippi", "north carolina", "south carolina", "tennessee", "virginia", "west virginia")) {
    merged_df$region[i] <- "southeast"
  } else if (merged_df$state[i] %in% c("arizona", "new mexico", "oklahoma", "texas")) {
    merged_df$region[i] <- "southwest"
  } else if (merged_df$state[i] %in% c("colorado", "idaho", "montana", "utah", "wyoming")) {
    merged_df$region[i] <- "rocky mountain"
  } else if (merged_df$state[i] %in% c("alaska", "california", "hawaii", "nevada", "oregon", "washington")) {
    merged_df$region[i] <- "far west"
  } else {
    merged_df$region[i] <- "other"
  }
}
```

```{r}
# Print all the unique values in the region column
unique(merged_df$region)
```

Now that the data in the dataframe matches the **`map_data()`** output, we appoint each state to the region it belongs to and then use **`ggplot()`** to draw the map:

```{r}
# Get the map of the United States
us_map <- map_data("state")

# Create a lookup table for state abbreviations and their corresponding full names
state_names <- data.frame(state = state.abb, name = tolower(state.name))

# Map the regions to the states
region_map <- us_map %>%
#left_join(state_names, by = c("region" = "state")) %>%
  left_join(state_names, by = c("region" = "name")) %>%
# merge(us_map, state_names, by.x=c("region"), by.y=c("name")) %>%
  mutate(region = 
           ifelse(region %in% c("connecticut", "maine", "massachusetts", "new hampshire", "rhode island", "vermont"), "New England",
                         ifelse(region %in% c("delaware", "maryland", "new jersey", "new york", "pennsylvania", "district of columbia"), "Mideast",
                                ifelse(region %in% c("illinois", "indiana", "michigan", "ohio", "wisconsin"), "Great Lakes",
                                       ifelse(region %in% c("iowa", "kansas", "minnesota", "missouri", "nebraska", "north dakota", "south dakota"), "Plains",
                                              ifelse(region %in% c("alabama", "arkansas", "florida", "georgia", "kentucky", "louisiana", "mississippi", "north carolina", "south carolina", "tennessee", "virginia", "west virginia"), "Southeast",
                                                     ifelse(region %in% c("arizona", "new mexico", "oklahoma", "texas"), "Southwest",
                                                            ifelse(region %in% c("colorado", "idaho", "montana", "utah", "wyoming"), "Rocky Mountain",
                                                                   ifelse(region %in% c("alaska", "california", "hawaii", "nevada", "oregon", "washington"), "Far West", NA
                                                                   )))))))))
                                                                          

# Summarize the data to get the center coordinates of each state
#state_centers <- region_map %>% 
#  group_by(state) %>% 
#  summarise(long = mean(long), lat = mean(lat))
# add labels
states <- aggregate(cbind(long, lat) ~ region, data=us_map, 
                FUN=function(x)mean(range(x)))
states$group <- c("AL", "AR", "AZ", "CA", "CO", "CT", "DE", "DC", "FL", "GA", "IA", 
              "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", 
              "MO", "MS", "MT", "NC", "ND", "NE", "NH", "NJ", "NM", "NV", "NY", 
              "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VA", 
              "VT", "WA", "WI", "WV", "WY")

# names(states)[names(states) == "region"] <- "group"


#Plot the map
ggplot(region_map, aes(x = long, y = lat, group = group, fill = region)) +
  geom_polygon(color = "black", show.legend = TRUE) +
#  geom_text(aes(label = state), data = region_map, size = 3, vjust = 2, hjust = 2) +
#  geom_text(aes(label = state), data = state_centers, size = 2, vjust = 2, hjust = 2) +
  geom_text(data = states, aes(long, lat, label = group), size = 2.5, inherit.aes = FALSE, color = "white", fontface = "bold") +
#  scale_fill_gradient(low = "white", high = "darkred") +
#  scale_fill_manual(values = viridis(n = 60), na.value = "gray") +
  labs(title = "Bureau of Economic Analysis Regional Divisions Map", fill = "Region") +
#  geom_text(aes(x = long, y = lat, label = state), data = state_centers, size = 3, color = "white") +
  theme_void() +
  theme(panel.background = element_rect(fill = "gray75", color = NA))


```

```{r}


```

```{r}

```

```{r}
# Group merged_df by region and calculate average microbusiness density
merged_df %>%
  group_by(region) %>%
  summarize(avg_density = mean(microbusiness_density)) %>%

  # Create bar plot of average density by region
  ggplot(aes(x = region, y = avg_density)) +
  geom_bar(stat = "identity") +
  
  # Add plot title and axis labels
  labs(title = "Average Microbusiness Density Per Region",
       x = "Region", y = "Avg Density") +
  
  # Apply a black and white theme to the plot
  theme_bw()
```

According to the above plot *New England* has the highest average microbusiness density, followed by *Farwest* and *Rocky Mountain* respectively, with a tiny difference, valuing more than 6.75. In contrast, *plains* has the lowest average microbusiness density, followed by *Southeast* and *Southwest*, all valued under 3.25. We can use a choropleth map to get a better view on the above information.

```{r}
# choropleth map
# Group merged_df by region and calculate average microbusiness density
avg_density <- merged_df %>%
  group_by(region) %>%
  summarize(avg_density = mean(microbusiness_density))

# Create a lookup table for state abbreviations and their corresponding full names
state_names <- data.frame(state = state.abb, name = tolower(state.name))

# Lowercase region column of region_map
region_map <- region_map %>% 
  mutate(region = tolower(region)) 

# Merge the average density data with the region_map data
plot_data <- merge(region_map, avg_density, by = "region") %>%
  arrange(order)

# Coordinates of the center of regions
bea_regions <- data.frame(
  group = c("New England", "Mideast", "Great Lakes", "Plains", 
             "Southeast", "Southwest", "Rocky Mountain", "Far West"),
  x = c(-71.8, -76.9, -86.6, -98.5, -82.4, -106.4, -111.1, -119.8),
  y = c(42.2, 39, 43.4, 39.8, 32.6, 34.3, 44.4, 38.4)
)

# Create the plot 
ggplot(plot_data, aes(x = long, y = lat, group = group, fill = avg_density)) +
  geom_polygon(color = "black") +
  geom_label(data = bea_regions,
             aes(x = x, y = y, label = group),
             size = 3, fontface = "bold", 
             label.padding = unit(0.2, "lines"),
             label.size = 0.2,
             fill = "gray75", color = "black") +
  
  scale_fill_gradient(low = "gray85", high = "darkred") +
  # scale_fill_viridis(name = "Avg Density", na.value = "gray") +
  labs(title = "Average Microbusiness Density Per Region", fill = "Avg Density") +
  theme_void() 
#  theme(panel.background = element_rect(fill = "gray90", color = NA))
```

Although, we can only see only one parameter on above map. To have a better look on the distribution, central tendency, spread, and variability of the `microbusiness_density` variable, we can use boxplots.

```{r}

# Aggregate data by region
df_by_region <- aggregate(microbusiness_density ~ region, merged_df, median)

# Create boxplot
ggplot(merged_df, aes(x = region, y = microbusiness_density)) +
  geom_boxplot() +
  labs(x = "Region", y = "Microbusiness Density") +
  # Arrange plots in grid
  facet_wrap(~ region, scales = "free", nrow = 2)

```

Above plots are not very informative because the small values are obscured by the larger ones. Therefore, using a logarithmic scale on the Y-axis can help to reduce this distortion and provide a more informative visualization of the data.

```{r}
# Create a grid of box plots
ggplot(merged_df, aes(x=region, y=microbusiness_density)) +
  geom_boxplot() +
  scale_y_log10() +
  labs(x="Region", y="Microbusiness Density") +
  ggtitle("Microbusiness Density by Region")

```

These boxplots are more informative, because using a logarithmic scale on the Y-axis helps to better reveal the differences and similarities between regions that helps to highlight any potential patterns or trends in the data. Some of the points that can be inferred from this boxplot include:

-   All regions have at least some microbusiness activity. Because minimum microbusiness density is greater than 0 in all regions.

-   The median microbusiness density is highest in the *New England* region, followed by the *Far West* and the *Mideast* regions.

-   The 3rd quartile microbusiness density is highest in the *Rocky Mountain* region, followed by the *New England* and the *Far West* regions.

-   The maximum microbusiness density is highest in the *Plains* region, followed by the *Rocky Mountain* and the *Far West* regions.

-   The mean microbusiness density is highest in the *New England* region, followed by the *Far West* and the *Rocky Mountain* regions.

-   The interquartile range (IQR = the difference between the 1st and 3rd quartiles) of microbusiness density is widest in the *Rocky Mountain* region, indicating that there is a greater range of microbusiness density in that region. In contrast, the IQR is narrowest in the *Plains* region.

```{r}
# by(merged_df$microbusiness_density, merged_df$region, summary)

```

```{r}
# Aggregate microbusiness density by state
state_avg <- aggregate(microbusiness_density ~ state, data = merged_df, FUN = mean)

# Load US map data
us_map <- map_data("state")

# Merge state_avg with us_map based on region and state
map_data <- merge(us_map, state_avg, by.x = "region", by.y = "state")

# Create a heatmap of microbusiness density by state
ggplot(map_data, aes(x = long, y = lat, group = group, fill = microbusiness_density)) +
  geom_polygon() +
  scale_fill_gradient(low = "white", high = "darkgreen") +
  coord_map() +
  labs(title = "Average Microbusiness Density per State", fill = "Density") +
  theme_void() +
  theme(panel.background = element_rect(fill = "lightblue", color = NA))
```

```{r}
# Create a grid of box plots
ggplot(merged_df, aes(x=state, y=microbusiness_density)) +
  geom_boxplot(colour = "darkblue", outlier.colour = "pink") +
  scale_y_log10() +
  labs(x="State", y="Microbusiness Density") +
  ggtitle("Microbusiness Density by State") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3))

```

```{r}
# {r fig.width = 10 ,fig.height = 12, out.width='100%', fig.align='center'}
# Aggregate microbusiness density by county

#county_avg <- merged_df %>%
#  group_by(cfips, county) %>%
#  summarise(microbusiness_density = mean(microbusiness_density))

county_avg <- aggregate(microbusiness_density ~ county + state, data = merged_df, FUN = mean)

# Get rid of county, city, and parish in the end of county names
county_avg$county <- gsub(" county", "", county_avg$county)
county_avg$county <- gsub(" city", "", county_avg$county)
county_avg$county <- gsub(" parish", "", county_avg$county)


# Load US county map data
us_map <- map_data("county")

# Merge county_avg with us_map based on region and county
map_data <- merge(us_map, county_avg, by.x = c("subregion", "region"), by.y = c("county", "state")) %>%
  arrange(order)

# Create a heatmap of microbusiness density by county using ggplot2
ggplot(map_data, aes(x = long, y = lat, group = group, fill = microbusiness_density)) +
  geom_polygon() +
  scale_fill_gradient(low = "lightblue", high = "navyblue") +
  coord_map() +
  labs(title = "Average Microbusiness Density per County", fill = "Density") +
  theme_void() +
  theme(panel.background = element_rect(fill = "gray85", color = NA))

```

```{r}
str(merged_df)
```

Boxplots are a visualization tool that provide insights into the central tendency and spread of a dataset, as well as identify outliers and skewness. They are useful for detecting anomalies and comparing variable distributions in a dataset, providing valuable insights into data distribution for exploratory data analysis.

```{r}
ggplot(merged_df, aes(y = microbusiness_density)) +
#  ggplot(merged_df, aes(x=region, y=microbusiness_density)) +
  geom_boxplot(colour = "blue") +
  scale_y_log10() +
  labs(y="Microbusiness Density") +
  ggtitle("Microbusiness Density") +
  theme_classic()
  
```

```{r fig.align='center', fig.height=9, fig.width=3}
# Create a grid of box plots
par(mfrow=c(3,2))
p1 <- ggplot(merged_df, aes(x=region, y=median_hh_inc)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="Region", y="Median Household Income") +
  ggtitle("Median Household Income by Region") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2 <- ggplot(merged_df, aes(x=region, y=pct_college)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="Region", y="Percentage with College Education") +
  ggtitle("Percentage with College Education by Region") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p3 <- ggplot(merged_df, aes(x=region, y=pct_foreign_born)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="Region", y="Percentage of Foreign-born Residents") +
  ggtitle("Percentage of Foreign-born Residents by Region") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p4 <- ggplot(merged_df, aes(x=region, y=pct_it_workers)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="Region", y="Percentage of IT Workers") +
  ggtitle("Percentage of IT Workers by Region") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p5 <- ggplot(merged_df, aes(x=region, y=pct_bb)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="Region", y="Percentage of Broadband Access") +
  ggtitle("Percentage of Broadband Access by Region") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


grid.arrange(p1, p2, p3, p4, p5, nrow = 5)


```

```{r fig.align='center', fig.height=9, fig.width=6}
# Create a grid of box plots

p1 <- ggplot(merged_df, aes(x=state, y=median_hh_inc)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="State", y="Median Household Income") +
  ggtitle("Median Household Income by State") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3))

p2 <- ggplot(merged_df, aes(x=state, y=pct_college)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="State", y="Percentage with College Education") +
  ggtitle("Percentage with College Education by State") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3))

p3 <- ggplot(merged_df, aes(x=state, y=pct_foreign_born)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="State", y="Percentage of Foreign-born Residents") +
  ggtitle("Percentage of Foreign-born Residents by State") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3))

p4 <- ggplot(merged_df, aes(x=state, y=pct_it_workers)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="State", y="Percentage of IT Workers") +
  ggtitle("Percentage of IT Workers by State") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3))

p5 <- ggplot(merged_df, aes(x=state, y=pct_bb)) +
  geom_boxplot(outlier.colour = "orange") +
  scale_y_log10() +
  labs(x="State", y="Percentage of Broadband Access") +
  ggtitle("Percentage of Broadband Access by State") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3))

grid.arrange(p1, p2, p3, p4, p5, nrow = 5)



```

```{r fig.width=6, fig.height=18}
# choropleth map with 5 variables
# Group merged_df by region and calculate average variables
avg_variables <- merged_df %>%
group_by(region) %>%
summarize(avg_pct_bb = mean(pct_bb),
avg_pct_college = mean(pct_college),
avg_pct_foreign_born = mean(pct_foreign_born),
avg_pct_it_workers = mean(pct_it_workers),
avg_median_hh_inc = mean(median_hh_inc))

# Create a lookup table for state abbreviations and their corresponding full names
state_names <- data.frame(state = state.abb, name = tolower(state.name))

# Lowercase region column of region_map
region_map <- region_map %>%
mutate(region = tolower(region))

# Merge the average density data with the region_map data
plot_data <- merge(region_map, avg_variables, by = "region") %>%
arrange(order)

# Coordinates of the center of regions
bea_regions <- data.frame(
group = c("New England", "Mideast", "Great Lakes", "Plains",
"Southeast", "Southwest", "Rocky Mountain", "Far West"),
x = c(-71.8, -76.9, -86.6, -98.5, -82.4, -106.4, -111.1, -119.8),
y = c(42.2, 39, 43.4, 39.8, 32.6, 34.3, 44.4, 38.4)
)

# Create the plot with a grid of 5 rows and 1 column
p1 <- ggplot(plot_data, aes(x = long, y = lat, group = group, fill = avg_pct_bb)) +
geom_polygon(color = "black") +
  geom_label(data = bea_regions,
             aes(x = x, y = y, label = group),
             size = 3, fontface = "bold", 
             label.padding = unit(0.2, "lines"),
             label.size = 0.2,
             fill = "gray75", color = "black") +
scale_fill_gradient(low = "gray85", high = "darkred") +
labs(title = "Average Percent of Broadband Access Per Region", fill = "Avg Density") +
theme_void()

p2 <- ggplot(plot_data, aes(x = long, y = lat, group = group, fill = avg_pct_college)) +
geom_polygon(color = "black") +
  geom_label(data = bea_regions,
             aes(x = x, y = y, label = group),
             size = 3, fontface = "bold", 
             label.padding = unit(0.2, "lines"),
             label.size = 0.2,
             fill = "gray75", color = "black") +
scale_fill_gradient(low = "gray85", high = "darkred") +
labs(title = "Average Percent of College Graduates Per Region", fill = "Avg Percent") +
theme_void()

p3 <- ggplot(plot_data, aes(x = long, y = lat, group = group, fill = avg_pct_foreign_born)) +
geom_polygon(color = "black") +
  geom_label(data = bea_regions,
             aes(x = x, y = y, label = group),
             size = 3, fontface = "bold", 
             label.padding = unit(0.2, "lines"),
             label.size = 0.2,
             fill = "gray75", color = "black") +
scale_fill_gradient(low = "gray85", high = "darkred") +
labs(title = "Average Percent of Foreign-Born Population Per Region", fill = "Avg Percent") +
theme_void()

p4 <- ggplot(plot_data, aes(x = long, y = lat, group = group, fill = avg_pct_it_workers)) +
geom_polygon(color = "black") +
  geom_label(data = bea_regions,
             aes(x = x, y = y, label = group),
             size = 3, fontface = "bold", 
             label.padding = unit(0.2, "lines"),
             label.size = 0.2,
             fill = "gray75", color = "black") +
scale_fill_gradient(low = "gray85", high = "darkred") +
labs(title = "Average Percent of IT Workers Per Region", fill = "Avg Percent") +
theme_void()

p5 <- ggplot(plot_data, aes(x = long, y = lat, group = group, fill = avg_median_hh_inc)) +
geom_polygon(color = "black") +
  geom_label(data = bea_regions,
             aes(x = x, y = y, label = group),
             size = 3, fontface = "bold", 
             label.padding = unit(0.2, "lines"),
             label.size = 0.2,
             fill = "gray75", color = "black") +
scale_fill_gradient(low = "gray85", high = "darkred") +
labs(title = "Average Median Household Income Per Region", fill = "Avg inc") +
theme_void()

grid.arrange(p1, p2, p3, p4, p5, nrow = 5)

```

```{r fig.width=4, fig.height=8}

state_avg <- aggregate(cbind(pct_bb, pct_college, pct_foreign_born, pct_it_workers, median_hh_inc) ~ state, data = merged_df, FUN = mean)


#Load US state map data
us_map <- map_data("state")

#Merge state_avg with us_map based on region and state
map_data <- merge(us_map, state_avg, by.x = c("region"), by.y = c("state")) %>%
arrange(order)

#Create a grid of heatmaps for each variable
grid_arrange_shared_legend <- function(...) {
plots <- list(...)
g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
lheight <- sum(legend$height)
grid.arrange(
do.call(arrangeGrob, lapply(plots, function(x)
x + theme(legend.position="none") + theme(panel.background = element_rect(fill = "gray85", color = NA)))
),
bottom = legend,
ncol = 5,
heights = rep((unit(1, "npc") - lheight) / length(plots), length(plots))
)
}

heatmap_bb <- ggplot(map_data, aes(x = long, y = lat, group = group, fill = pct_bb)) +
  geom_polygon() +
  scale_fill_gradient(low = "white", high = "darkgreen") +
  coord_map() +
  labs(title = "Percent of Broadband Access per State", fill = "Percent") +
  theme_void()

heatmap_college <- 
  ggplot(map_data, aes(x = long, y = lat, group = group, fill = pct_college)) +
  geom_polygon() +
  scale_fill_gradient(low = "white", high = "darkgreen") +
  coord_map() +
  labs(title = "Percent of Population with College Education per State", fill = "Percent") +
  theme_void()

heatmap_foreign_born <- 
  ggplot(map_data, aes(x = long, y = lat, group = group, fill = pct_foreign_born)) +
  geom_polygon() +
  scale_fill_gradient(low = "white", high = "darkgreen") +
  coord_map() +
  labs(title = "Percent of Foreign-born Population per State", fill = "Percent") +
  theme_void()

heatmap_it_workers <- 
  ggplot(map_data, aes(x = long, y = lat, group = group, fill = pct_it_workers)) +
  geom_polygon() +
  scale_fill_gradient(low = "white", high = "darkgreen") +
  coord_map() +
  labs(title = "Percent of IT Workers per State", fill = "Percent") +
  theme_void()
heatmap_inc <- 
  ggplot(map_data, aes(x = long, y = lat, group = group, fill = median_hh_inc)) +
  geom_polygon() +
  scale_fill_gradient(low = "white", high = "darkgreen") +
  coord_map() +
  labs(title = "Median Household Income per State", fill = "Dollars") +
  theme_void()

#Plot the grid of heatmaps
#grid_arrange_shared_legend(heatmap_bb, heatmap_college, heatmap_foreign_born, heatmap_it_workers, heatmap_inc)
grid.arrange(heatmap_bb, heatmap_college, heatmap_foreign_born, heatmap_it_workers, heatmap_inc, nrow = 5)

```

```{r fig.width=4, fig.height=8}

county_avg <- aggregate(cbind(pct_bb, pct_college, pct_foreign_born, pct_it_workers, median_hh_inc) ~ county + state, data = merged_df, FUN = mean)

#Get rid of county, city, and parish in the end of county names
county_avg$county <- gsub(" county", "", county_avg$county)
county_avg$county <- gsub(" city", "", county_avg$county)
county_avg$county <- gsub(" parish", "", county_avg$county)

#Load US county map data
us_map <- map_data("county")

#Merge county_avg with us_map based on region and county
map_data <- merge(us_map, county_avg, by.x = c("subregion", "region"), by.y = c("county", "state")) %>%
arrange(order)

#Create a grid of heatmaps for each variable
grid_arrange_shared_legend <- function(...) {
plots <- list(...)
g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
lheight <- sum(legend$height)
grid.arrange(
do.call(arrangeGrob, lapply(plots, function(x)
x + theme(legend.position="none") + theme(panel.background = element_rect(fill = "gray85", color = NA)))
),
bottom = legend,
ncol = 5,
heights = rep((unit(1, "npc") - lheight) / length(plots), length(plots))
)
}

heatmap_bb <- ggplot(map_data, aes(x = long, y = lat, group = group, fill = pct_bb)) +
  geom_polygon() +
  scale_fill_gradient(low = "lightblue", high = "navyblue") +
  coord_map() +
  labs(title = "Percent of Broadband Access per County", fill = "Percent") +
  theme_void()

heatmap_college <- 
  ggplot(map_data, aes(x = long, y = lat, group = group, fill = pct_college)) +
  geom_polygon() +
  scale_fill_gradient(low = "lightblue", high = "navyblue") +
  coord_map() +
  labs(title = "Percent of Population with College Education per County", fill = "Percent") +
  theme_void()

heatmap_foreign_born <- 
  ggplot(map_data, aes(x = long, y = lat, group = group, fill = pct_foreign_born)) +
  geom_polygon() +
  scale_fill_gradient(low = "lightblue", high = "navyblue") +
  coord_map() +
  labs(title = "Percent of Foreign-born Population per County", fill = "Percent") +
  theme_void()

heatmap_it_workers <- 
  ggplot(map_data, aes(x = long, y = lat, group = group, fill = pct_it_workers)) +
  geom_polygon() +
  scale_fill_gradient(low = "lightblue", high = "navyblue") +
  coord_map() +
  labs(title = "Percent of IT Workers per County", fill = "Percent") +
  theme_void()
heatmap_inc <- 
  ggplot(map_data, aes(x = long, y = lat, group = group, fill = median_hh_inc)) +
  geom_polygon() +
  scale_fill_gradient(low = "lightblue", high = "navyblue") +
  coord_map() +
  labs(title = "Median Household Income per County", fill = "Dollars") +
  theme_void()

#Plot the grid of heatmaps
#grid_arrange_shared_legend(heatmap_bb, heatmap_college, heatmap_foreign_born, heatmap_it_workers, heatmap_inc)
grid.arrange(heatmap_bb, heatmap_college, heatmap_foreign_born, heatmap_it_workers, heatmap_inc, nrow = 5)

```

```{r fig.height=5, fig.width=6}
par(mfrow=c(2,3)) # set plot layout to 3 rows and 2 columns
boxplot(merged_df$median_hh_inc, col = "pink", main = "Median Household Income")
boxplot(merged_df$pct_college, col = "pink", main = "Percentage with College Education")
boxplot(merged_df$pct_foreign_born, col = "pink", main = "Percentage of Foreign-born Residents")
boxplot(merged_df$pct_it_workers, col = "pink", main = "Percentage of IT Workers")
boxplot(merged_df$pct_bb, col = "pink", main = "Percentage of Broadband Access")
#boxplot(merged_df$active, col = "pink", main = "Active Microbusiness Count")

```

### 

## 3.8. Outlier Detection

Outlier detection is an important step in data analysis, as outliers can significantly affect the results of statistical analyses. There are several methods to detect outliers depending on the distribution of the data.

The Shapiro-Wilk normality test is a statistical test used to determine if a given dataset follows a normal distribution. We will perform a Shapiro-Wilk normality test on a random sample of 5000 observations from the `microbusiness_density` column of **merged_df** dataframe. To run this test we first set the seed value using **`set.seed()`** function to a specific random seed to ensure that the results are reproducible if the code is run again. Then, we weill use the **`shapiro.test()`** function to perform the Shapiro-Wilk normality test on the **sample_data** object. The function returns the test statistic (W) and the p-value. A W value closer to 1 indicates that the data is more normally distributed, while a W value closer to 0 indicates greater deviation from normality. If the p-value is less than the significance level (typically 0.05), then the null hypothesis (that the sample data is normally distributed) is rejected in favor of the alternative hypothesis (that the sample data is not normally distributed).

```{r}
# Sample 5000 observations from microbusiness_density column
set.seed(92) # Set seed for reproducibility
sample_data <- sample(merged_df$microbusiness_density, 5000)

# Perform Shapiro-Wilk test on sample_data
shapiro.test(sample_data)

```

The test resulted in a W statistic of 0.55179 and a p-value of less than 2.2e-16. Based on the results of the Shapiro-Wilk normality test, it can be concluded that the **sample_data** is not normally distributed.

To have a better visual on distribution of the data we will use a bell curve, a boxplot, and a Q-Q plot on `microbusiness_density`.

```{r fig.width=12, fig.height=4}
# Calculate the mean and standard deviation of 'microbusiness_density'
mean_density <- mean(merged_df$microbusiness_density)
sd_density <- sd(merged_df$microbusiness_density)

# Create a range of values for the x-axis
x_values <- seq(mean_density - 3*sd_density, mean_density + 3*sd_density, length.out = 1000)

# Create a bell curve with mean and standard deviation calculated above
y_values <- dnorm(x_values, mean = mean_density, sd = sd_density)

# Combine the 'x_values' and 'y_values' into a data frame
density_df <- data.frame(x = x_values, y = y_values)

# Create a boxplot 
boxplot <- ggplot(data = merged_df, aes(x = "", y = merged_df$microbusiness_density)) +
  geom_boxplot(fill = "skyblue") +
  scale_y_log10() +
  labs(x = "", y = "Microbusiness Density (logarithmic)") +
  ggtitle("Boxplot for Microbusiness Density")

# Create a Q-Q plot and add a diagonal line
qqplot <- ggplot(data = merged_df, aes(sample = microbusiness_density)) + 
  stat_qq() + 
  stat_qq_line(colour = "red") +
  labs(x = "Theoretical Normal Quantiles", y = "Observed Quantiles") +
  ggtitle("Q-Q Plot for Microbusiness Density")

# Create a histogram and add the bell curve
densityplot <- ggplot(data = merged_df, aes(x = microbusiness_density)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, colour = "black", fill = "white") +
  geom_line(data = density_df, aes(x = x, y = y), colour = "red", linewidth = 1) +
  labs(x = "Microbusiness Density", y = "Density") +
  ggtitle("Distribution of Microbusiness Density")

# Arrange the plots in one row using the 'grid.arrange' function from the 'gridExtra' package
grid.arrange(densityplot, boxplot, qqplot, ncol = 3, widths = c(2, 1, 2))

```

The above distribution plot explains that the dataset is right-skewed. The boxplot shows some data points away from the upper whisker; hence outliers are present in `microbusiness_density`. Q-Q plot's alignment is away from the 45-degree angle depicting outliers in the dataset.

Since the data is right-skewed and not normally distributed, a common approach to detecting outliers is to use the interquartile range (IQR) method. Now, we have to find the boundary of minimum and maximum values, out of which data would be considered an outlier.

The decision range approach involves setting a range of values outside of which any observations are considered outliers. One common approach is to use the interquartile range (IQR) to define the decision range. The IQR is calculated as the difference between the third quartile (Q3) and the first quartile (Q1) of the data.

The decision range is then defined as the range from $Q1 - 1.5 * IQR$ to $Q3 + 1.5 * IQR$. Any observations that fall outside of this range are considered outliers. This method is useful for identifying potential outliers in a dataset and can help to ensure that statistical analyses are robust and accurate.

```{r}
quartiles <- quantile(merged_df$microbusiness_density, probs = seq(0, 1, 0.25), na.rm = FALSE,
         names = TRUE, type = 7, digits = 6)
quartiles
```

```{r}
# Calculate IQR of microbusiness_density column
q <- quantile(merged_df$microbusiness_density, c(0.25, 0.75))
iqr <- q[2] - q[1]

# Calculate lower and upper bounds for outliers
lower_bound <- q[1] - 1.5*iqr
upper_bound <- q[2] + 1.5*iqr

# Count number of outliers
num_outliers <- sum(merged_df$microbusiness_density < lower_bound | merged_df$microbusiness_density > upper_bound)

# Calculate percent of outliers
percent_outliers <- num_outliers / length(merged_df$microbusiness_density) * 100

# Print results
cat("Number of outliers:", num_outliers, "\n")
cat("Percent of outliers:", percent_outliers, "%\n")
```

```{r}
# Create new dataframe without outliers
merged_df_new <- merged_df[merged_df$microbusiness_density >= lower_bound & merged_df$microbusiness_density <= upper_bound,]

# Print number of rows removed
cat("Number of rows removed:", nrow(merged_df) - nrow(merged_df_new), "\n")

```

```{r fig.width=12, fig.height=4}
# Calculate the mean and standard deviation of 'microbusiness_density'
mean_density <- mean(merged_df_new$microbusiness_density)
sd_density <- sd(merged_df_new$microbusiness_density)

# Create a range of values for the x-axis
x_values <- seq(mean_density - 3*sd_density, mean_density + 3*sd_density, length.out = 1000)

# Create a bell curve with mean and standard deviation calculated above
y_values <- dnorm(x_values, mean = mean_density, sd = sd_density)

# Combine the 'x_values' and 'y_values' into a data frame
density_df <- data.frame(x = x_values, y = y_values)

# Create a boxplot
boxplot <- ggplot(data = merged_df_new, aes(x = "", y = merged_df_new$microbusiness_density)) +
  geom_boxplot(fill = "skyblue") +
#  scale_y_log10() +
  labs(x = "", y = "Microbusiness Density") +
  ggtitle("Boxplot for Microbusiness Density")

# Create a Q-Q plot and add a diagonal line
qqplot <- ggplot(data = merged_df_new, aes(sample = microbusiness_density)) + 
  stat_qq() + 
  stat_qq_line(colour = "red") +
  labs(x = "Theoretical Normal Quantiles", y = "Observed Quantiles") +
  ggtitle("Q-Q Plot for Microbusiness Density")

# Create a bell curve and add the bell curve 
densityplot <- ggplot(data = merged_df_new, aes(x = microbusiness_density)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, colour = "black", fill = "white") +
  geom_line(data = density_df, aes(x = x, y = y), colour = "red", linewidth = 1) +
  labs(x = "Microbusiness Density", y = "Density") +
  ggtitle("Distribution of Microbusiness Density")

# Arrange the plots in one row
grid.arrange(densityplot, boxplot, qqplot, ncol = 3, widths = c(2, 1, 2))

```

```{r}
# Calculate IQR of microbusiness_density column
q <- quantile(merged_df_new$microbusiness_density, c(0.25, 0.75))
iqr <- q[2] - q[1]

# Calculate lower and upper bounds for outliers
lower_bound <- q[1] - 1.5*iqr
upper_bound <- q[2] + 1.5*iqr

# Count number of outliers
num_outliers <- sum(merged_df_new$microbusiness_density < lower_bound | merged_df_new$microbusiness_density > upper_bound)

# Calculate percent of outliers
percent_outliers <- num_outliers / length(merged_df_new$microbusiness_density) * 100

# Print results
cat("Number of outliers:", num_outliers, "\n")
cat("Percent of outliers:", percent_outliers, "%\n")
```

```{r}
# Create new dataframe without outliers
merged_df_clean <- merged_df_new[merged_df_new$microbusiness_density >= lower_bound & merged_df_new$microbusiness_density <= upper_bound,]

# Print number of rows removed
cat("Number of rows removed:", nrow(merged_df_new) - nrow(merged_df_clean), "\n")
cat("Total rows removed:", nrow(merged_df) - nrow(merged_df_clean))
```

```{r fig.width=12, fig.height=4}
# Calculate the mean and standard deviation of 'microbusiness_density'
mean_density <- mean(merged_df_clean$microbusiness_density)
sd_density <- sd(merged_df_clean$microbusiness_density)

# Create a range of values for the x-axis
x_values <- seq(mean_density - 3*sd_density, mean_density + 3*sd_density, length.out = 1000)

# Create a bell curve with mean and standard deviation calculated above
y_values <- dnorm(x_values, mean = mean_density, sd = sd_density)

# Combine the 'x_values' and 'y_values' into a data frame
density_df <- data.frame(x = x_values, y = y_values)

# Create a boxplot
boxplot <- ggplot(data = merged_df_clean, aes(x = "", y = merged_df_clean$microbusiness_density)) +
  geom_boxplot(fill = "skyblue") +
#  scale_y_log10() +
  labs(x = "", y = "Microbusiness Density") +
  ggtitle("Boxplot for Microbusiness Density")

# Create a Q-Q plot and add a diagonal line
qqplot <- ggplot(data = merged_df_clean, aes(sample = microbusiness_density)) + 
  stat_qq() + 
  stat_qq_line(colour = "red") +
  labs(x = "Theoretical Normal Quantiles", y = "Observed Quantiles") +
  ggtitle("Q-Q Plot for Microbusiness Density")

# Create a histogram and add the bell curve
densityplot <- ggplot(data = merged_df_clean, aes(x = microbusiness_density)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, colour = "black", fill = "white") +
  geom_line(data = density_df, aes(x = x, y = y), colour = "red", linewidth = 1) +
  labs(x = "Microbusiness Density", y = "Density") +
  ggtitle("Distribution of Microbusiness Density")

# Arrange the plots in one row using the 'grid.arrange' function from the 'gridExtra' package
grid.arrange(densityplot, boxplot, qqplot, ncol = 3, widths = c(2, 1, 2))

```

```{r fig.width=12, fig.height=4}
# Calculate the mean and standard deviation of 'pct_bb'
mean_pct_bb <- mean(merged_df$pct_bb)
sd_pct_bb <- sd(merged_df$pct_bb)

# Create a range of values for the x-axis
x_values <- seq(mean_pct_bb - 3*sd_pct_bb, mean_pct_bb + 3*sd_pct_bb, length.out = 1000)

# Create a bell curve with mean and standard deviation calculated above
y_values <- dnorm(x_values, mean = mean_pct_bb, sd = sd_pct_bb)

# Combine the 'x_values' and 'y_values' into a data frame
pct_bb_df <- data.frame(x = x_values, y = y_values)

# Create a boxplot
boxplot <- ggplot(data = merged_df, aes(x = "", y = merged_df$pct_bb)) +
  geom_boxplot(fill = "skyblue") +
  scale_y_log10() +
  labs(x = "", y = "Microbusiness pct_bb (logarithmic)") +
  ggtitle("Boxplot for Microbusiness pct_bb")

# Create a Q-Q plot and add a diagonal line
qqplot <- ggplot(data = merged_df, aes(sample = pct_bb)) + 
  stat_qq() + 
  stat_qq_line(colour = "red") +
  labs(x = "Theoretical Normal Quantiles", y = "Observed Quantiles") +
  ggtitle("Q-Q Plot for Microbusiness pct_bb")

# Create a histogram and add the bell curve
densityplot <- ggplot(data = merged_df, aes(x = pct_bb)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, colour = "black", fill = "white") +
  geom_line(data = pct_bb_df, aes(x = x, y = y), colour = "red", linewidth = 1) +
  labs(x = "Microbusiness pct_bb", y = "pct_bb") +
  ggtitle("Distribution of Microbusiness pct_bb")

# Arrange the plots in one row using the 'grid.arrange' function from the 'gridExtra' package
grid.arrange(densityplot, boxplot, qqplot, ncol = 3, widths = c(2, 1, 2))

```

```{r}
# Specify the columns to analyze
cols_to_analyze <- c("pct_bb", "pct_college", "pct_it_workers", "pct_foreign_born", "median_hh_inc")

# Loop through each column and detect outliers using the IQR method
for (col in cols_to_analyze) {
  # Calculate the interquartile range (IQR) of the column
  q1 <- quantile(merged_df_new[[col]], 0.25)
  q3 <- quantile(merged_df_new[[col]], 0.75)
  iqr <- q3 - q1
  
  # Calculate the upper and lower bounds for outliers
  upper_bound <- q3 + (1.5 * iqr)
  lower_bound <- q1 - (1.5 * iqr)
  
  # Identify the outliers in the column
  outliers <- merged_df_new[[col]][merged_df_new[[col]] > upper_bound | merged_df_new[[col]] < lower_bound]
  
  # Print the results
#  cat(paste("Outliers in", col, ":", toString(outliers), "\n"))
  # Count number of outliers
  num_outliers <- sum(merged_df_new[[col]] < lower_bound | merged_df_new[[col]] > upper_bound)
  # Calculate percent of outliers
  percent_outliers <- num_outliers / length(merged_df_new[[col]]) * 100

# Print results
cat("Number of outliers in", col,":", num_outliers, "\n")
cat("Percent of outliers in", col,":", percent_outliers, "%\n")
}

```

```{r}
# Create a copy of the original dataframe
merged_df_clean <- merged_df_new

# Loop through each column and detect outliers using the IQR method
for (col in cols_to_analyze) {
  # Calculate the interquartile range (IQR) of the column
  q1 <- quantile(merged_df_clean[[col]], 0.25)
  q3 <- quantile(merged_df_clean[[col]], 0.75)
  iqr <- q3 - q1
  
  # Calculate the upper and lower bounds for outliers
  upper_bound <- q3 + (1.5 * iqr)
  lower_bound <- q1 - (1.5 * iqr)
  
  # Identify the outliers in the column
  outliers <- merged_df_clean[[col]][merged_df_clean[[col]] > upper_bound | merged_df_clean[[col]] < lower_bound]
  
  # Remove the outliers from the dataframe
  merged_df_clean <- merged_df_clean[!(merged_df_clean[[col]] %in% outliers), ]
}

# Print the length of the original and cleaned dataframes
cat("Original dataframe length:", nrow(merged_df_new), "\n")
cat("Cleaned dataframe length:", nrow(merged_df_clean), "\n")
cat("Total rows removed:", nrow(merged_df) - nrow(merged_df_clean), "\n")
cat("Total percent removed:", 100 * (nrow(merged_df) - nrow(merged_df_clean)) / nrow(merged_df),"%", "\n")
```

## 3.9. Non-Parametric Test

```{r}
# Specify a list of column pairs to compare
col_pairs <- list(c("microbusiness_density", "pct_bb"), c("microbusiness_density", "pct_college"), c("microbusiness_density", "pct_foreign_born"), c("microbusiness_density", "pct_it_workers"), c("microbusiness_density", "median_hh_inc"))

# Loop through each column pair and perform the Wilcoxon signed-rank test
for (pair in col_pairs) {
  test_result <- wilcox.test(merged_df_new[[pair[1]]], merged_df_new[[pair[2]]], paired = TRUE)
  cat(paste("Wilcoxon signed-rank test results for", pair[1], "and", pair[2], ":\n"))
  print(test_result)
  cat("\n")
}


```

```{r}
# Specify a list of column pairs to compare
col_pairs <- list(c("microbusiness_density", "pct_bb"), c("microbusiness_density", "pct_college"), c("microbusiness_density", "pct_foreign_born"), c("microbusiness_density", "pct_it_workers"), c("microbusiness_density", "median_hh_inc"))

# Loop through each column pair and perform the Wilcoxon rank sum test
for (pair in col_pairs) {
  test_result <- wilcox.test(merged_df_new[[pair[1]]], merged_df_new[[pair[2]]], paired = FALSE)
  cat(paste("Wilcoxon rank-sum test results for", pair[1], "and", pair[2], ":\n"))
  print(test_result)
  cat("\n")
}


```

The results show that for each pair of columns, the p-value is less than the significance level of 0.05, which means that we reject the null hypothesis that the median difference between the two columns is zero. Instead, we conclude that there is a statistically significant difference between the two columns.

The results suggest that there is evidence to support the hypothesis that the population median of each column is different from the population median of microbusiness_density.

kernel regression

```{r}
# create a vector of region names and their corresponding codes
region_codes <- c("new england" = 1, 
                   "mideast" = 2, 
                   "great lakes" = 3, 
                   "plains" = 4, 
                   "southeast" = 5, 
                   "southwest" = 6, 
                   "rocky mountain" = 7, 
                   "far west" = 8)

# use the `match()` function to find the region code for each row in `merged_df_new$region`
merged_df_new$region_code <- match(merged_df_new$region, names(region_codes))
merged_df_clean$region_code <- match(merged_df_clean$region, names(region_codes))


# create a vector of state names and their corresponding codes
state_codes <- sort(unique(merged_df_new$state))
state_codes <- setNames(1:length(state_codes), state_codes)

# use the `match()` function to find the state code for each row in `merged_df_new$state`
merged_df_new$state_code <- match(merged_df_new$state, names(state_codes))

state_codes <- sort(unique(merged_df_clean$state))
state_codes <- setNames(1:length(state_codes), state_codes)
merged_df_clean$state_code <- match(merged_df_clean$state, names(state_codes))

# print head of the updated dataframe
head(merged_df_new)
```

```{r}
merged_df_new |>
  GGally::ggpairs(columns = c(17,18,2))
```

```{r}

# Select the columns with numeric data
numeric_cols <- c("cfips", "state_code", "region_code", "microbusiness_density", "active", "year", "month", "pct_bb", "pct_college", "pct_foreign_born", "pct_it_workers", "median_hh_inc")

# Subset the dataframe with the selected columns
merged_df_numeric <- merged_df_new[, numeric_cols]

# Calculate the correlation matrix
cor_matrix <- cor(merged_df_numeric, use="pairwise.complete.obs")

# Plot the correlation matrix using ggcorrplot
ggcorrplot(cor_matrix, 
           hc.order = TRUE, 
           type = "lower", 
           method = "square",
           lab = TRUE, 
           lab_size = 3, 
           title = "Correlation Plot of merged_df_new", 
           colors = c("#6D9EC1", "#FAC200", "#FA5252"), 
           ggtheme = ggplot2::theme_gray, 
           show.legend = TRUE)

```

The above correlation plot belongs to our merged dataframe after removing the outliers from `microbusiness_density` column. The below correlation plot belongs to our merged dataframe after removing the outliers from the columns we imported from the **imputed_data** dataframe.

```{r}

# Select the columns with numeric data
numeric_cols <- c("cfips", "state_code", "region_code", "microbusiness_density", "active", "year", "month", "pct_bb", "pct_college", "pct_foreign_born", "pct_it_workers", "median_hh_inc")

# Subset the dataframe with the selected columns
merged_df_numeric <- merged_df_clean[, numeric_cols]

# Calculate the correlation matrix
cor_matrix <- cor(merged_df_numeric, use="pairwise.complete.obs")

# Plot the correlation matrix using ggcorrplot
ggcorrplot(cor_matrix, 
           hc.order = TRUE, 
           type = "lower", 
           method = "square",
           lab = TRUE, 
           lab_size = 3, 
           title = "Correlation Plot of merged_df_clean", 
           colors = c("#6D9EC1", "#FAC200", "#FA5252"), 
           ggtheme = ggplot2::theme_gray, 
           show.legend = TRUE)

```

The correlation percentage decreased after removing the outliers from our dataframe. The `cfips` column and `state_code` correlate completely positively. So, we will remove the redundant feature `state_code`. Also, `median_hh_inc` strongly correlates with `pct_bb` and `pct_college`. Also, `pct_college` has a moderately strong correlation with `microbusiness_density`, `median_hh_inc`, and `pct_bb`. Since features with strong correlations are redundant, we will remove `median_hh_inc` before further analysis. Still, we will not remove `pct_college` because there is a limited number of features, and this feature might be useful in forecasting the `microbusiness_density` value. Now, we will draw the new correlation plot.

```{r}

# Select the columns with numeric data (not including "state_code", "median_hh_inc", )
numeric_cols <- c("cfips",  "region_code", "microbusiness_density", "active", "year", "month", "pct_college", "pct_foreign_born", "pct_it_workers", "pct_bb")

# Subset the dataframe with the selected columns
merged_df_numeric <- merged_df_clean[, numeric_cols]

# Calculate the correlation matrix
cor_matrix <- cor(merged_df_numeric, use="pairwise.complete.obs")

# Plot the correlation matrix using ggcorrplot
ggcorrplot(cor_matrix, 
           hc.order = TRUE, 
           type = "lower", 
           method = "square",
           lab = TRUE, 
           lab_size = 3, 
           title = "Correlation Plot of merged_df_clean", 
           colors = c("#6D9EC1", "#FAC200", "#FA5252"), 
           ggtheme = ggplot2::theme_gray, 
           show.legend = TRUE)

```

## 3.10. Feature Importance Using Random Forest

Random Forest is a machine learning algorithm that is often used for classification and regression tasks. It is a type of ensemble learning method that combines the results of multiple decision trees to improve the accuracy of predictions. One of the advantages of Random Forest is that it provides a measure of feature importance that can be used to identify which variables are the most important for the model's performance. Feature importance using Random Forest is a powerful tool for analyzing and interpreting the predictive models and can be used for identifying the most important features for predicting the `microbusiness_density`.

```{r}

# Split the dataset into training and testing sets
set.seed(92) # for reproducibility
train_index <- sample(nrow(merged_df_new), 0.7 * nrow(merged_df_new))
train_data <- merged_df_new[train_index, ]
test_data <- merged_df_new[-train_index, ]

# Build the random forest model
model <- randomForest(microbusiness_density ~ cfips + region_code + active + year + month + pct_bb + pct_college + pct_foreign_born + pct_it_workers + median_hh_inc, 
                      data = merged_df_clean)

# Make predictions on the test set
#predictions <- predict(model, test_data)

# Check the accuracy of the model
# accuracy <- sum(predictions == test_data$microbusiness_density) / nrow(test_data)
# cat("Accuracy:", accuracy, "\n")

# Plot the variable importance
varImpPlot(model, main = "Variable Importance Plot")

# Print the variable importance scores
varImp(model)

```

The variable importance scores indicate how much each predictor contributes to the accuracy of the model in predicting the outcome. The higher the importance score, the more important the predictor is in the model. This output indicates that the most important predictors for predicting `microbusiness_density` are `active`, `pct_college`, `pct_foreign_born`, `pct_bb` and `cfips`, while `year` and `month` have the least impact on the model's performance. We will use these results to decide which predictors to include in the model and which ones to exclude.

```{r}
# Load revealed_test.csv into a dataframe
revealed_test_df <- read.csv("./datasets/revealed_test.csv")

# Change first_day_of_month format in "revealed_test_df" to Date
revealed_test_df$first_day_of_month <- as.Date(revealed_test_df$first_day_of_month)
```

# 4. Time Series Forecasting

Based on the feature importance scores provided by the **Random Forest** model, we can identify which variables are most strongly associated with `microbusiness_density`. The variables that have the highest importance scores are likely to be the most important predictors of microbusiness_density. In this case, the variables with the highest importance scores are: - active (116924.3551) - pct_college (57310.5642) - pct_foreign_born (27024.1637) - pct_bb (24301.8238) - cfips (23658.8595) - region_code (17026.2925) - median_hh_inc (22442.3452) - pct_it_workers (14892.8354) Therefore, we can frame our forecasting problem as a cross-sectional data problem. To frame microbusiness_density forecasting as a cross-sectional data problem, we will use these variables as predictors in a statistical model. We will use the **merged_df_clean** dataset that includes these variables as well as `microbusiness_density` for a particular month and county. The goal would be to build a model that predicts the `microbusiness_density` of the county for that specific month, based on the other variables in the dataset.

## 4.1 Exponential Smoothing Forecasting

Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in industry.

### 4.1.1. Simple Exponential Smoothing Forecast

Exponential smoothing is a general technique for smoothing time series data by giving more weight to recent observations. The simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern. (There is a grow in the last few years, which might suggest a trend. We will consider whether a trended method would be better for this series later.)

```{r}
# Split the dataset into training and testing sets
set.seed(92) # for reproducibility
train_index <- sample(nrow(merged_df_clean), 0.7 * nrow(merged_df_clean))
train_data <- merged_df_clean[train_index, ]
test_data <- merged_df_clean[-train_index, ]

# Build the random forest model
model <- randomForest(microbusiness_density ~ cfips + active + pct_bb + pct_college + pct_foreign_born, 
                      data = merged_df_clean)

# Make predictions on the test set
predictions <- predict(model, test_data)

```

```{r}
# Select relevant predictor columns
# predictors <- merged_df_new[, c("microbusiness_density", "cfips")]

merged_df_ts <- merged_df_clean %>%
  group_by(first_day_of_month) %>%
  summarise(microbusiness_density = mean(microbusiness_density))

# Convert data to time series format
ts_data <- ts(merged_df_ts$microbusiness_density, start = c(2019, 8), frequency = 12)
```

```{r}
# fit a simple exponential smoothing model to the time series data
fit <- ets(ts_data, model = "ANN")


# generate forecasts for the time series data using the fitted model
forecast_values <- forecast::forecast(fit, h = 8)

```

```{r}
plot(forecast_values, main = "Simple Exponential Smoothing Forecast", xlab = "Year", ylab = "Microbusiness Density")
lines(forecast_values$fitted, col = "red")
lines(ts_data, col = "blue")
legend("bottomleft", legend = c("Forecast", "Actual"), col = c("red", "blue"), lty = 1)

```

The forecasts for the period 11/2022 to 06/2023 are plotted in above. Also, plotted are one-step-ahead fitted values alongside the data over the period 08/2019 to 10/2022.

This confusion matrix shows the performance of the random forest model in predicting the microbusiness density for each category in the test set. The rows represent the predicted categories, and the columns represent the actual categories.

For example, the value in the first row and first column (24) represents the number of test data points that were predicted to be in category 0 (i.e., a microbusiness density of 0) and were actually in category 0. The value in the second row and first column (272) represents the number of test data points that were predicted to be in category 1 (i.e., a microbusiness density of 1) and were actually in category 0.

From the matrix, we can see that the model performed well for some classes (e.g., classes 0 and 9), but had more difficulty with others (e.g., classes 3, 4, and 5). We can also see that there were very few cases in some classes (e.g., class 9).

```{r}
write.csv(merged_df_clean, "merged_df_clean.csv")
```

```{r fig.dim = c(12, 20)}
# Load the rpart package

# Subset the dataframe to include only relevant columns
df <- merged_df_clean[, c("cfips", "pct_bb", "pct_college", "active", "microbusiness_density")]

# Split the data into training and testing sets
set.seed(92) # set seed for reproducibility
train_idx <- sample(nrow(df), size = round(0.7 * nrow(df)), replace = FALSE)
train <- df[train_idx, ]
test <- df[-train_idx, ]

# Build the decision tree model using training data
tree <- rpart(microbusiness_density ~ active + pct_bb + pct_college, data = train)

# Visualize the decision tree
plot(tree)
text(tree)

# Make predictions on the testing data
pred <- predict(tree, newdata = test)

# Calculate the mean squared error of the predictions
mse <- mean((test$microbusiness_density - pred)^2)
mse

```

```{r}
print(tree)
```

```{r}
# Predict the partition for each row in the dataframe
tree_pred <- predict(tree, newdata = merged_df_clean)

# Create a factor variable based on the predicted values
partition <- as.factor(tree_pred)

# Split the dataframe into multiple sub-dataframes based on the partition variable
partitions <- split(merged_df_clean, partition)

```

```{r}
#merged_df_clean$first_day_of_month <- as.Date(merged_df_clean$first_day_of_month)
str(merged_df_clean)
colnames(merged_df_clean)
```
